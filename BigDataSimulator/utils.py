# utils.py
import graphviz

# --- 1. ARAYÃœZ METÄ°NLERÄ° ---
UI_TEXTS = {
    "en": {
        "sidebar_title": "Stack Builder",
        "select_instr": "Select components from each layer below:",
        "generate_btn": "âœ¨ Auto-Generate Architecture",
        "reset_btn": "ğŸ—‘ï¸ Clear All",
        "manifest": "ğŸ“ Pipeline Manifest",
        "success_gen": "Architecture generated with {n} components.",
        "warning_no_tech": "Please select at least 2 technologies.",
        "error_missing_dep": "âŒ **Missing Dependency:** `{tech}` requires `{dep}` to function.",
        "layer_ingest": "Ingestion Layer",
        "layer_store": "Storage / Lake Layer",
        "layer_proc": "Processing / Transformation",
        "layer_db": "Serving / Warehouse / NoSQL",
        "layer_bi": "BI / Visualization Layer",
        "orch_note": "Orchestration & Infrastructure",
        "modernity": "Status",
        "doc_link": "Official Docs",
        "dependency": "âš ï¸ Dependency Warning"
    },
    "tr": {
        "sidebar_title": "YÄ±ÄŸÄ±n OluÅŸturucu",
        "select_instr": "AÅŸaÄŸÄ±daki katmanlardan bileÅŸenlerinizi seÃ§in:",
        "generate_btn": "âœ¨ Otomatik Mimari OluÅŸtur",
        "reset_btn": "ğŸ—‘ï¸ Hepsini Temizle",
        "manifest": "ğŸ“ Mimari Ã–zeti",
        "success_gen": "{n} bileÅŸen ile mimari oluÅŸturuldu.",
        "warning_no_tech": "LÃ¼tfen en az 2 teknoloji seÃ§in.",
        "error_missing_dep": "âŒ **Eksik BaÄŸÄ±mlÄ±lÄ±k:** `{tech}` teknolojisinin Ã§alÄ±ÅŸmasÄ± iÃ§in `{dep}` gereklidir.",
        "layer_ingest": "Veri AlÄ±m (Ingestion) KatmanÄ±",
        "layer_store": "Depolama / GÃ¶l KatmanÄ±",
        "layer_proc": "Ä°ÅŸleme / DÃ¶nÃ¼ÅŸtÃ¼rme KatmanÄ±",
        "layer_db": "VeritabanÄ± / Ambar / NoSQL",
        "layer_bi": "BI / GÃ¶rselleÅŸtirme KatmanÄ±",
        "orch_note": "Orkestrasyon ve AltyapÄ±",
        "modernity": "Durum",
        "doc_link": "Resmi DokÃ¼man",
        "dependency": "âš ï¸ BaÄŸÄ±mlÄ±lÄ±k UyarÄ±sÄ±"
    }
}

LAYER_PRIORITY = {
    "Ingestion": 1, "Storage": 2, "Lakehouse": 2,
    "Processing": 3, "Databases": 4, "Serving/BI": 5,
    "Orchestration": 0, "AI/ML": 4
}

DEPENDENCY_RULES = {
    "Kafka": ["Zookeeper"], "Hadoop MR": ["HDFS", "YARN"], "ClickHouse": ["Zookeeper"],
    "Delta Lake": ["Spark"], "Airflow": ["PostgreSQL"], "HBase": ["HDFS", "Zookeeper"],
    "Hive": ["HDFS", "Hadoop MR"], "Flink": ["Zookeeper"], "dbt": ["Snowflake"],
    "Kubernetes": ["Docker"], "Ozone": ["Hadoop MR"], "Trino": ["S3"],
    "Kubeflow": ["Kubernetes"], "Storm": ["Zookeeper"], "Spark MLlib": ["Spark"],
    "cAdvisor": ["Docker"], "Kibana": ["Elasticsearch"], "Pulsar": ["Zookeeper"],
    "Debezium": ["Kafka"], "Sqoop": ["Hadoop MR"] # Yeni Kurallar
}
# --- DEVASA TEKNOLOJÄ° ANSÄ°KLOPEDÄ°SÄ° ---
TECH_STACK = {
"Ingestion": {
        "Kafka": {
            "desc": {"en": "Distributed Event Streaming.", "tr": "DaÄŸÄ±tÄ±k Olay AkÄ±ÅŸ Platformu."},
            "detail": {
                "en": """### 1. Definition
Apache Kafka is an open-source, distributed event streaming platform developed for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.

### 2. Core Purpose
The primary goal is to process large volumes of data streams in real-time with high throughput and low latency. Unlike traditional queues, it persists data on disk, reducing coupling between systems.

### 3. Architecture
Kafka operates on a distributed "Commit Log" logic.
* **Write:** Data producers send data to the Kafka cluster.
* **Storage:** Kafka keeps this data on disk sequentially.
* **Read:** Consumers can read data from any point in the stream.

### 4. Components
Producer, Consumer, Broker, Topic, Partition, Offset, Consumer Group, ZooKeeper/KRaft.

### 5. Use Cases
Log Aggregation, Stream Processing, Event Sourcing.""",
                "tr": """### 1. TanÄ±m
Apache Kafka; yÃ¼ksek performanslÄ± veri hatlarÄ±, akÄ±ÅŸ analitiÄŸi ve veri entegrasyonu iÃ§in geliÅŸtirilmiÅŸ; aÃ§Ä±k kaynaklÄ±, daÄŸÄ±tÄ±k bir olay akÄ±ÅŸ platformudur.

### 2. Temel AmaÃ§
BÃ¼yÃ¼k hacimli veri akÄ±ÅŸlarÄ±nÄ± gerÃ§ek zamanlÄ± olarak, yÃ¼ksek iÅŸleme kapasitesi ve dÃ¼ÅŸÃ¼k gecikme ile iÅŸlemektir. Verileri bellekte deÄŸil diskte kalÄ±cÄ± saklar.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Kafka, daÄŸÄ±tÄ±k bir "Commit Log" (Ä°ÅŸlem GÃ¼nlÃ¼ÄŸÃ¼) mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸÄ±r. Veriler, sÄ±ralÄ± ve deÄŸiÅŸtirilemez kayÄ±tlar olarak diske yazÄ±lÄ±r.
* **Yazma:** Veri Ã¼reticileri, veriyi Kafka kÃ¼mesine gÃ¶nderir.
* **Saklama:** Kafka bu veriyi konfigÃ¼re edilen sÃ¼re boyunca diskte tutar.
* **Okuma:** TÃ¼keticiler, kaldÄ±klarÄ± yerden veriyi okuyabilirler.

### 4. Temel BileÅŸenler
Producer, Consumer, Broker, Topic (Konu), Partition (BÃ¶lÃ¼m), Offset, Consumer Group, ZooKeeper/KRaft.

### 5. KullanÄ±m AlanlarÄ±
Log Toplama, AkÄ±ÅŸ Ä°ÅŸleme, Event Sourcing, Mikroservis Ä°letiÅŸimi."""
            },
            "link": "https://kafka.apache.org/", "modern": True, "dep": "Zookeeper",
            "code": "producer.send('my-topic', b'Hello World')"
        },
        "Redpanda": {
            "desc": {"en": "C++ Kafka Alternative.", "tr": "Modern C++ Kafka Alternatifi."},
            "detail": {
                "en": """### 1. Definition
Redpanda is a Kafka-compatible streaming platform written in C++. It utilizes a thread-per-core architecture to bypass JVM latency.

### 2. Core Purpose
To maintain high performance while eliminating ZooKeeper dependency and JVM complexity.

### 3. Architecture
Uses Seastar framework for thread-per-core architecture. No Zookeeper (uses internal Raft).""",
                "tr": """### 1. TanÄ±m
Redpanda; modern donanÄ±mlar iÃ§in optimize edilmiÅŸ, C++ ile yazÄ±lmÄ±ÅŸ, Kafka API uyumlu, yÃ¼ksek performanslÄ± bir olay akÄ±ÅŸ platformudur.

### 2. Temel AmaÃ§
Kafka'nÄ±n sunduÄŸu yÃ¼ksek veri iÅŸleme kapasitesini korurken, ZooKeeper ve JVM karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± ortadan kaldÄ±rmaktÄ±r.

### 3. Mimari
Thread-per-Core mimarisini kullanÄ±r. JVM Ã¼zerinde Ã§alÄ±ÅŸmaz, doÄŸrudan donanÄ±ma eriÅŸir. Kendi iÃ§inde Raft algoritmasÄ±nÄ± barÄ±ndÄ±rÄ±r."""
            },
            "link": "https://redpanda.com/", "modern": True, "dep": None,
            "code": "docker run -d --name redpanda -p 9092:9092 vectorized/redpanda start"
        },
        "Logstash": {
            "desc": {"en": "Server-side Data Pipeline.", "tr": "Sunucu TaraflÄ± Veri Ä°ÅŸleme HattÄ±."},
            "detail": {
                "en": """### 1. Definition
Logstash is an open-source server-side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a "stash" like Elasticsearch. Part of the ELK Stack.

### 2. Core Purpose
To normalize data from different sources (especially logs) into a single format and make it analyzable. It uses "Grok" filters to parse unstructured text into structured fields.

### 3. Architecture
It uses a three-stage pipeline:
* **Input:** Ingests data (File, Syslog, Kafka, Http).
* **Filter:** Processes, enriches, and formats data (The strongest part).
* **Output:** Writes data to the destination (Elasticsearch, Email, File).

### 4. Key Components
* **Pipeline:** Definition of data flow.
* **Grok:** Filter that parses log lines using Regex patterns.
* **Plugins:** Hundreds of input, filter, and output plugins.

### 5. Use Cases
ELK Stack logging, Security data (SIEM) enrichment.

### 6. Pros and Cons
* **Pros:** Powerful transformation (Grok), Flexibility.
* **Cons:** Resource Consumption (JVM based, high RAM usage), Performance bottlenecks in high traffic.""",
                "tr": """### 1. TanÄ±m
Logstash; Elastic (ELK) yÄ±ÄŸÄ±nÄ±nÄ±n bir parÃ§asÄ± olan, veriyi anÄ±nda toplayan, dÃ¶nÃ¼ÅŸtÃ¼ren ve istenilen hedefe ("stash") gÃ¶nderen, sunucu taraflÄ± bir aÃ§Ä±k kaynaklÄ± veri iÅŸleme hattÄ±dÄ±r.

### 2. Temel AmaÃ§
FarklÄ± kaynaklardan gelen verileri (Ã¶zellikle loglarÄ±) tek bir formatta normalize etmek ve analiz edilebilir hale getirmektir. "Grok" filtreleri sayesinde karmaÅŸÄ±k yapÄ±daki metinleri anlamlÄ± alanlara bÃ¶ler.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
ÃœÃ§ aÅŸamalÄ± bir iÅŸlem hattÄ± kullanÄ±r:
* **Input:** Veriyi alÄ±r (Dosya, Syslog, Kafka, Http vb.).
* **Filter:** Veriyi iÅŸler, zenginleÅŸtirir veya formatlar.
* **Output:** Veriyi hedefe yazar (Elasticsearch, Email, File).

### 4. Temel BileÅŸenler
Pipeline, Grok (Regex filtresi), Eklentiler (Plugins).

### 5. KullanÄ±m AlanlarÄ±
ELK Stack log ayrÄ±ÅŸtÄ±rma, SIEM verilerinin zenginleÅŸtirilmesi.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** GÃ¼Ã§lÃ¼ DÃ¶nÃ¼ÅŸÃ¼m, Esneklik.
* **Dezavantajlar:** Kaynak TÃ¼ketimi (Java tabanlÄ±dÄ±r, fazla RAM tÃ¼ketir)."""
            },
            "link": "https://www.elastic.co/logstash", "modern": False, "dep": None,
            "code": """# logstash.conf
input {
  file { path => "/var/log/syslog" }
}
filter {
  grok { match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname}" } }
}
output {
  elasticsearch { hosts => ["localhost:9200"] }
}"""
        },
        "Fluentd": {
            "desc": {"en": "Unified Logging Layer.", "tr": "BirleÅŸik Loglama KatmanÄ±."},
            "detail": {
                "en": """### 1. Definition
Fluentd is an open-source, cloud-native data collector for unified logging. It is the standard logger for Kubernetes environments (CNCF project).

### 2. Core Purpose
To solve the "n x m" complexity between data sources and backend systems; structuring log data in JSON format and transporting it.

### 3. Architecture
Written in C and Ruby. Consumes very few resources. Uses a Pluggable architecture. Routes data using **Tags**.
* **Input:** Collects data.
* **Parser:** Converts data to JSON.
* **Buffer:** Buffers data against network failures.
* **Output:** Sends data to destination.

### 4. Key Components
Input Plugins, Parser, Buffer, Output Plugins.

### 5. Use Cases
Kubernetes & Docker logging, IoT data collection (Fluent Bit).

### 6. Pros and Cons
* **Pros:** Lightweight, Cloud-Native standard (CNCF), JSON based.
* **Cons:** Ruby dependency for some plugins, Complex configuration.""",
                "tr": """### 1. TanÄ±m
Fluentd; veri toplama ve tÃ¼ketimi birleÅŸtiren, aÃ§Ä±k kaynaklÄ±, Cloud-Native (Bulut Yerlisi) bir veri toplayÄ±cÄ±dÄ±r. Ã–zellikle Kubernetes ortamlarÄ±nÄ±n standart loglayÄ±cÄ±sÄ±dÄ±r.

### 2. Temel AmaÃ§
Veri kaynaklarÄ± ile arka uÃ§ sistemleri arasÄ±ndaki karmaÅŸÄ±klÄ±ÄŸÄ± Ã§Ã¶zmek; log verilerini JSON formatÄ±nda yapÄ±landÄ±rarak taÅŸÄ±maktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
C ve Ruby ile yazÄ±lmÄ±ÅŸtÄ±r. Az kaynak tÃ¼ketir. Veriyi **Etiketler (Tag)** kullanarak yÃ¶nlendirir.
* **Input Plugins:** Veriyi toplar (tail, http).
* **Parser:** Veriyi JSON'a Ã§evirir.
* **Buffer:** AÄŸ kesintilerine karÅŸÄ± veriyi tamponlar.
* **Output Plugins:** Veriyi hedefe gÃ¶nderir.

### 4. Temel BileÅŸenler
Girdi Eklentileri, AyrÄ±ÅŸtÄ±rÄ±cÄ±, Tampon, Ã‡Ä±ktÄ± Eklentileri.

### 5. KullanÄ±m AlanlarÄ±
Kubernetes & Docker loglarÄ±, IoT cihazlarÄ± (Fluent Bit).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Hafiflik, Ekosistem (CNCF standardÄ±).
* **Dezavantajlar:** Ruby BaÄŸÄ±mlÄ±lÄ±ÄŸÄ±, KonfigÃ¼rasyon zorluÄŸu."""
            },
            "link": "https://www.fluentd.org/", "modern": True, "dep": None,
            "code": """# fluent.conf
<source>
  @type tail
  path /var/log/httpd-access.log
  tag apache.access
</source>

<match apache.**>
  @type stdout
</match>"""
        },
        "Debezium": {
            "desc": {"en": "Change Data Capture (CDC).", "tr": "Veri DeÄŸiÅŸikliÄŸi Yakalama (CDC)."},
            "detail": {
                "en": """### 1. Definition
Debezium is a distributed platform for Change Data Capture (CDC), built on top of Apache Kafka Connect. It captures row-level changes in databases.

### 2. Core Purpose
To allow applications to respond almost immediately to database changes (inserts, updates, deletes) without polling. "Turning the database into an event stream."

### 3. Architecture
Debezium reads the database's **Transaction Logs** (e.g., Binlog for MySQL, WAL for Postgres). It converts each change into a JSON message and writes it to a Kafka Topic. This adds no overhead to the database queries.

### 4. Key Components
* **Kafka Connect:** The runtime environment.
* **Connectors:** Database-specific connectors (MySQL, Postgres, Oracle).
* **Schema Registry:** Manages schema evolution.

### 5. Use Cases
Cache Invalidation, Search Indexing, Microservices Data Exchange (Strangler Fig Pattern).

### 6. Pros and Cons
* **Pros:** Zero Data Loss (reads logs), Low Overhead (No polling).
* **Cons:** Complex Management (Needs Kafka), Schema Evolution handling.""",
                "tr": """### 1. TanÄ±m
Debezium; veritabanlarÄ±ndaki deÄŸiÅŸiklikleri anlÄ±k olarak yakalayan (CDC) ve bunlarÄ± bir olay akÄ±ÅŸÄ±na dÃ¶nÃ¼ÅŸtÃ¼ren, Kafka Connect Ã¼zerine inÅŸa edilmiÅŸ daÄŸÄ±tÄ±k bir platformdur.

### 2. Temel AmaÃ§
UygulamalarÄ±n veritabanÄ±nÄ± sÃ¼rekli sorgulamasÄ±na (polling) gerek kalmadan; veri deÄŸiÅŸtiÄŸinde anÄ±nda haberdar olmasÄ±nÄ± saÄŸlamaktÄ±r. "VeritabanÄ±nÄ± bir olay akÄ±ÅŸÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmek" olarak Ã¶zetlenir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Debezium, veritabanÄ±nÄ±n **Ä°ÅŸlem GÃ¼nlÃ¼klerini (Transaction Logs)** okur (Ã–rn: MySQL Binlog, Postgres WAL). Bu loglarÄ± okuyarak her deÄŸiÅŸikliÄŸi bir JSON mesajÄ± olarak Kafka Topic'ine yazar. DoÄŸrudan sorgu atmadÄ±ÄŸÄ± iÃ§in veritabanÄ±nÄ± yormaz.

### 4. Temel BileÅŸenler
Kafka Connect (Ã‡alÄ±ÅŸma ortamÄ±), Connectors (BaÄŸlayÄ±cÄ±lar), Schema Registry.

### 5. KullanÄ±m AlanlarÄ±
Cache Temizleme (Redis), Arama Ä°ndeksleme (Elasticsearch), Mikroservis geÃ§iÅŸleri.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** SÄ±fÄ±r Veri KaybÄ±, DÃ¼ÅŸÃ¼k Kaynak TÃ¼ketimi.
* **Dezavantajlar:** YÃ¶netim ZorluÄŸu (Kafka baÄŸÄ±mlÄ±lÄ±ÄŸÄ±), Åema DeÄŸiÅŸimleri."""
            },
            "link": "https://debezium.io/", "modern": True, "dep": "Kafka",
            "code": """{
  "name": "inventory-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "database.server.name": "dbserver1",
    "database.include.list": "inventory"
  }
}"""
        },
        "Sqoop": {
            "desc": {"en": "SQL to Hadoop Transfer.", "tr": "SQL'den Hadoop'a Veri AktarÄ±mÄ±."},
            "detail": {
                "en": """### 1. Definition
Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases (RDBMS).

### 2. Core Purpose
In the early days of Big Data, it was used to offload enterprise data (Oracle, MySQL) to Hadoop for analysis.

### 3. Architecture
It works on **MapReduce**. It reads database metadata, creates "Mapper" tasks to parallelize the transfer, and writes data to HDFS via JDBC. (No Reduce phase).

### 4. Key Components
Import Tool (RDBMS -> HDFS), Export Tool (HDFS -> RDBMS), JDBC Drivers.

### 5. Use Cases
Data Warehouse Offloading, Nightly Batch Jobs. (Note: Retired project, but still in use in legacy systems).

### 6. Pros and Cons
* **Pros:** Parallelism (Fast bulk transfer), Integration (Hive/HBase creation).
* **Cons:** Legacy Architecture (High latency due to MapReduce), No Streaming support.""",
                "tr": """### 1. TanÄ±m
Apache Sqoop ("SQL to Hadoop"); iliÅŸkisel veritabanlarÄ± (RDBMS) ile Hadoop ekosistemi (HDFS, Hive, HBase) arasÄ±nda toplu veri aktarÄ±mÄ± (bulk transfer) yapmak iÃ§in tasarlanmÄ±ÅŸ bir araÃ§tÄ±r.

### 2. Temel AmaÃ§
BÃ¼yÃ¼k veri dÃ¼nyasÄ±nÄ±n ilk dÃ¶nemlerinde, kurumsal verilerin (Oracle, MySQL) Hadoop ortamÄ±na analiz iÃ§in taÅŸÄ±nmasÄ±nÄ± saÄŸlamaktÄ±.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
**MapReduce** tabanlÄ± Ã§alÄ±ÅŸÄ±r. VeritabanÄ± metadatasÄ±nÄ± okur, aktarÄ±mÄ± paralelleÅŸtirmek iÃ§in "Mapper" gÃ¶revleri oluÅŸturur ve veriyi JDBC Ã¼zerinden parÃ§alar halinde Ã§ekerek HDFS'e yazar.

### 4. Temel BileÅŸenler
Import Tool, Export Tool, JDBC SÃ¼rÃ¼cÃ¼leri.

### 5. KullanÄ±m AlanlarÄ±
Veri ambarÄ± verilerini Hadoop'a arÅŸivlemek, Gece Ã§alÄ±ÅŸan toplu iÅŸler (Batch Jobs).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Paralellik (YÃ¼ksek hÄ±z), Hive entegrasyonu.
* **Dezavantajlar:** Eski Mimari (Hantal MapReduce), GerÃ§ek zamanlÄ± (Streaming) desteÄŸi yok."""
            },
            "link": "https://sqoop.apache.org/", "modern": False, "dep": "Hadoop MR",
            "code": """# MySQL'den HDFS'e veri Ã§ekme
sqoop import \\
  --connect jdbc:mysql://localhost/db \\
  --username root \\
  --table employees \\
  --target-dir /user/hadoop/employees \\
  -m 1"""
        },
        "RabbitMQ": {
            "desc": {"en": "Traditional Message Broker.", "tr": "Geleneksel Mesaj KuyruÄŸu."},
            "detail": {
                "en": """### 1. Definition
RabbitMQ is an open-source, traditional message broker that implements the AMQP standard with advanced routing capabilities.

### 2. Core Purpose
To manage complex messaging scenarios between applications and make tasks asynchronous. Unlike Kafka, it focuses on "queue" logic.

### 3. Architecture
Operates on "Smart Broker, Dumb Consumer" principle. Uses Exchanges to route messages to Queues based on Bindings.

### 4. Components
Exchange, Queue, Binding, Erlang Runtime.

### 5. Use Cases
Background tasks, Complex routing, Order processing.

### 6. Pros and Cons
* **Pros:** Flexible Routing, Push Model.
* **Cons:** Lower throughput than Kafka, Messages deleted after consumption.""",
                "tr": """### 1. TanÄ±m
RabbitMQ; geliÅŸmiÅŸ yÃ¶nlendirme (routing) yeteneklerine sahip, AMQP standardÄ±nÄ± uygulayan, aÃ§Ä±k kaynaklÄ±, geleneksel bir mesaj aracÄ±sÄ±dÄ±r (Message Broker).

### 2. Temel AmaÃ§
Uygulamalar arasÄ±nda karmaÅŸÄ±k mesajlaÅŸma senaryolarÄ±nÄ± yÃ¶netmek ve gÃ¶revleri asenkron hale getirmektir. Kafka'nÄ±n aksine "kuyruk" mantÄ±ÄŸÄ±na odaklanÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
"AkÄ±llÄ± Sunucu, Aptal TÃ¼ketici" prensibiyle Ã§alÄ±ÅŸÄ±r. Ãœretici mesajÄ± bir "Exchange"e gÃ¶nderir, Exchange kurallara gÃ¶re mesajÄ± ilgili kuyruklara (Queue) daÄŸÄ±tÄ±r.

### 4. Temel BileÅŸenler
Exchange, Queue, Binding, Erlang Runtime.

### 5. KullanÄ±m AlanlarÄ±
Arka plan iÅŸlemleri, KarmaÅŸÄ±k yÃ¶nlendirme gerektiren haberleÅŸmeler.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Esnek YÃ¶nlendirme, Push Modeli.
* **Dezavantajlar:** Performans (Kafka'dan dÃ¼ÅŸÃ¼ktÃ¼r), Veri Saklama (TÃ¼ketilen silinir)."""
            },
            "link": "https://www.rabbitmq.com/", "modern": False, "dep": "Erlang",
            "code": """import pika
connection = pika.BlockingConnection()
channel = connection.channel()
channel.basic_publish(exchange='', routing_key='hello', body='Hello!')"""
        },
        "Pulsar": {
            "desc": {"en": "Cloud-Native Messaging.", "tr": "Bulut TabanlÄ± MesajlaÅŸma."},
            "detail": {
                "en": """### 1. Definition
Apache Pulsar is a distributed pub-sub platform designed for cloud-native architectures, combining messaging and streaming.

### 2. Core Purpose
To solve Kafka's scaling challenges by providing multi-tenancy and separation of compute/storage.

### 3. Architecture
Separation of Compute (Stateless Brokers) and Storage (BookKeeper). Allows independent scaling.

### 4. Components
Broker, BookKeeper, ZooKeeper, Pulsar Functions.

### 5. Use Cases
SaaS platforms, Geo-replication, Queuing + Streaming hybrid.

### 6. Pros and Cons
* **Pros:** Tiered Storage, Multi-tenancy.
* **Cons:** Architectural Complexity, Smaller community.""",
                "tr": """### 1. TanÄ±m
Apache Pulsar; bulut tabanlÄ± mimariler iÃ§in tasarlanmÄ±ÅŸ, hem mesajlaÅŸma hem de olay akÄ±ÅŸÄ± Ã¶zelliklerini tek Ã§atÄ±da toplayan platformdur.

### 2. Temel AmaÃ§
Kafka'nÄ±n Ã¶lÃ§eklenme zorluklarÄ±nÄ± Ã§Ã¶zmek ve Ã§ok kiracÄ±lÄ± (multi-tenant) yapÄ± sunmaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
En belirgin Ã¶zelliÄŸi "Hesaplama ve DepolamanÄ±n AyrÄ±lmasÄ±"dÄ±r. Broker'lar veriyi iÅŸlemez, BookKeeper veriyi saklar.

### 4. Temel BileÅŸenler
Broker, BookKeeper, ZooKeeper, Pulsar Functions.

### 5. KullanÄ±m AlanlarÄ±
BÃ¼yÃ¼k Ã¶lÃ§ekli bulut uygulamalarÄ±, CoÄŸrafi daÄŸÄ±tÄ±k sistemler.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** S3'e otomatik veri taÅŸÄ±ma (Tiered Storage), BaÄŸÄ±msÄ±z Ã¶lÃ§eklenme.
* **Dezavantajlar:** Mimari karmaÅŸÄ±klÄ±k, PopÃ¼larite."""
            },
            "link": "https://pulsar.apache.org/", "modern": True, "dep": "Zookeeper",
            "code": """import pulsar
client = pulsar.Client('pulsar://localhost:6650')
producer = client.create_producer('my-topic')
producer.send(('Hello').encode('utf-8'))"""
        },
        "Airbyte": {
            "desc": {"en": "ELT Data Integration.", "tr": "AÃ§Ä±k Kaynak ELT."},
            "detail": {
                "en": """### 1. Definition
Airbyte is an open-source data integration (ELT) platform to extract data from APIs/DBs and load to warehouses.

### 2. Core Purpose
To easily pull data from thousands of "Long-tail" SaaS applications. Open-source alternative to Fivetran.

### 3. Architecture
Container-based (Docker). Runs connectors as isolated containers. Follows ELT (Extract-Load-Transform).

### 4. Components
Source Connectors, Destination Connectors, Worker, Scheduler.

### 5. Use Cases
Marketing data consolidation, DB replication.

### 6. Pros and Cons
* **Pros:** Huge library, Open Source.
* **Cons:** Management overhead, Performance at massive scale.""",
                "tr": """### 1. TanÄ±m
Airbyte; verileri API'lerden ve veritabanlarÄ±ndan alÄ±p veri ambarlarÄ±na taÅŸÄ±mak iÃ§in kullanÄ±lan aÃ§Ä±k kaynaklÄ± ELT platformudur.

### 2. Temel AmaÃ§
Binlerce farklÄ± SaaS uygulamasÄ±ndan veriyi kolayca Ã§ekebilmek. Fivetran'Ä±n aÃ§Ä±k kaynaklÄ± alternatifidir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Konteyner tabanlÄ±dÄ±r. Her baÄŸlayÄ±cÄ± izole bir Docker konteyneri olarak Ã§alÄ±ÅŸÄ±r. ELT (Ã‡Ä±kar-YÃ¼kle-DÃ¶nÃ¼ÅŸtÃ¼r) mantÄ±ÄŸÄ±nÄ± izler.

### 4. Temel BileÅŸenler
Kaynak BaÄŸlayÄ±cÄ±larÄ±, Hedef BaÄŸlayÄ±cÄ±larÄ±, Worker, Scheduler.

### 5. KullanÄ±m AlanlarÄ±
Pazarlama verisi toplama, Modern Veri YÄ±ÄŸÄ±nÄ± kurulumlarÄ±.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** GeniÅŸ KonektÃ¶r KÃ¼tÃ¼phanesi, AÃ§Ä±k Kaynak.
* **Dezavantajlar:** Kendi sunucunda yÃ¶netim zorluÄŸu."""
            },
            "link": "https://airbyte.com/", "modern": True, "dep": "Docker",
            "code": "# Airbyte API Call"
        },
        "Fivetran": {
            "desc": {"en": "Managed ELT.", "tr": "YÃ¶netilen ELT."},
            "detail": {
                "en": """### 1. Definition
Fully managed automated data movement platform.

### 2. Core Purpose
Zero-maintenance pipelines. Handles schema drift automatically.

### 3. Architecture
SaaS (Software as a Service). No infrastructure to manage.

### 4. Components
Connectors, Dashboard.

### 5. Use Cases
Enterprise data ingestion without engineering overhead.

### 6. Pros and Cons
* **Pros:** Reliability, Ease of use.
* **Cons:** Cost, Closed source.""",
                "tr": """### 1. TanÄ±m
Tamamen yÃ¶netilen, otomatik veri taÅŸÄ±ma platformu.

### 2. Temel AmaÃ§
BakÄ±m gerektirmeyen boru hatlarÄ±. Åema deÄŸiÅŸikliklerini otomatik yÃ¶netir.

### 3. Mimari
SaaS modelidir. AltyapÄ± yÃ¶netimi yoktur.

### 4. Temel BileÅŸenler
BaÄŸlayÄ±cÄ±lar, YÃ¶netim Paneli.

### 5. KullanÄ±m AlanlarÄ±
MÃ¼hendislik eforu harcamadan kurumsal veri taÅŸÄ±ma.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** GÃ¼venilirlik, KolaylÄ±k.
* **Dezavantajlar:** Maliyet, KapalÄ± kaynak."""
            },
            "link": "https://www.fivetran.com/", "modern": True, "dep": None,
            "code": "# No Code - Managed Service"
        },
        "NiFi": {
            "desc": {"en": "Data Flow Automation.", "tr": "GÃ¶rsel Veri AkÄ±ÅŸ Otomasyonu."},
            "detail": {
                "en": """### 1. Definition
Apache NiFi is a visual flow-based programming tool for automating data flow. Developed by NSA.

### 2. Core Purpose
Visually design routing, transformation, and mediation of data.

### 3. Architecture
Flow-based. Handles **Backpressure** automatically.

### 4. Components
FlowFile, Processor, Process Group, Flow Controller.

### 5. Use Cases
IoT data collection, Legacy migration.

### 6. Pros and Cons
* **Pros:** Visual Interface, Data Provenance.
* **Cons:** Small file problem, Stateful.""",
                "tr": """### 1. TanÄ±m
Apache NiFi; veri akÄ±ÅŸÄ±nÄ± otomatize etmek iÃ§in tasarlanmÄ±ÅŸ, gÃ¶rsel arayÃ¼ze sahip bir araÃ§tÄ±r. NSA tarafÄ±ndan geliÅŸtirilmiÅŸtir.

### 2. Temel AmaÃ§
Kod yazmadan veri akÄ±ÅŸlarÄ±nÄ± gÃ¶rsel olarak tasarlamak ve yÃ¶netmek.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
AkÄ±ÅŸ tabanlÄ±dÄ±r. Hedef sistem yavaÅŸlarsa "Geri BasÄ±nÃ§" (Backpressure) uygulayarak akÄ±ÅŸÄ± yavaÅŸlatÄ±r.

### 4. Temel BileÅŸenler
FlowFile, Processor, Process Group, Flow Controller.

### 5. KullanÄ±m AlanlarÄ±
IoT veri toplama, Eski sistemlerden gÃ¶Ã§.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** GÃ¶rsel ArayÃ¼z, Veri Soy AÄŸacÄ±.
* **Dezavantajlar:** KÃ¼Ã§Ã¼k dosya sorunu, Durumlu (Stateful) yapÄ±."""
            },
            "link": "https://nifi.apache.org/", "modern": True, "dep": "Zookeeper",
            "code": "# Visual Interface"
        },
        "Kinesis": {
            "desc": {"en": "AWS Streaming.", "tr": "AWS AkÄ±ÅŸ Servisi."},
            "detail": {
                "en": """### 1. Definition
Serverless streaming service on AWS.

### 2. Core Purpose
Real-time data streaming without managing servers. Kafka alternative on AWS.

### 3. Architecture
Uses Shards instead of Partitions. Fully managed.

### 4. Components
Data Streams, Firehose, Analytics.

### 5. Use Cases
AWS-centric real-time apps.

### 6. Pros and Cons
* **Pros:** Serverless, AWS Integration.
* **Cons:** Vendor Lock-in, Cost at scale.""",
                "tr": """### 1. TanÄ±m
AWS Ã¼zerinde sunulan sunucusuz veri akÄ±ÅŸ servisi.

### 2. Temel AmaÃ§
Sunucu yÃ¶netimi olmadan gerÃ§ek zamanlÄ± veri akÄ±ÅŸÄ± saÄŸlamak.

### 3. Mimari
Partition yerine "Shard" kullanÄ±r. Tamamen yÃ¶netilen bir servistir.

### 4. Temel BileÅŸenler
Data Streams, Firehose.

### 5. KullanÄ±m AlanlarÄ±
AWS odaklÄ± gerÃ§ek zamanlÄ± uygulamalar.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Sunucusuz, AWS Entegrasyonu.
* **Dezavantajlar:** SaÄŸlayÄ±cÄ± baÄŸÄ±mlÄ±lÄ±ÄŸÄ± (Vendor Lock-in), YÃ¼ksek Ã¶lÃ§ekte maliyet."""
            },
            "link": "https://aws.amazon.com/kinesis/", "modern": True, "dep": "AWS",
            "code": "import boto3\nkinesis = boto3.client('kinesis')"
        }
    },
    "Storage": {
        "S3": {
            "desc": {"en": "Object Storage Standard.", "tr": "Nesne Depolama StandardÄ±."},
            "detail": {
                "en": """### 1. Definition
Amazon S3 is an object storage service designed to store data at internet scale. It is the de facto standard and API reference point for the data storage world today.

### 2. Core Purpose
To store any amount of data (structured or unstructured) with 99.999999999% (11 9s) durability and make it accessible over the internet. Unlike classic file systems (hierarchical folder structure), it stores data in a flat addressing space, ensuring infinite scalability.

### 3. Architecture and Working Principle
S3 stores data as "Objects" inside containers called "Buckets".
* **Key-Value:** Each file has a unique key (URL).
* **Flat Structure:** There are no actual folders, it behaves like folders using prefix logic (e.g., logs/2023/file.txt is a single key).
* **Replication:** Data is automatically replicated to at least 3 physically separate data centers (Availability Zones) within a Region.

### 4. Key Components
* **Bucket:** The top-level container where objects are stored.
* **Object:** The data itself (File) + Metadata + Key (ID).
* **Storage Classes:** Classes providing cost optimization based on access frequency (Standard, Glacier, Deep Archive).

### 5. Use Cases
Data Lake (Big Data analytics center), Backup and Archive, Static Website Hosting.

### 6. Pros and Cons
* **Pros:** Infinite Scale, Extreme Durability (11 9s), Universal Integration.
* **Cons:** Latency (Slower than local disks), Egress Cost (Data transfer fees).""",

                "tr": """### 1. TanÄ±m
Amazon S3; internet Ã¶lÃ§eÄŸinde veri depolamak iÃ§in tasarlanmÄ±ÅŸ, nesne tabanlÄ± (object storage) bir bulut depolama servisidir. GÃ¼nÃ¼mÃ¼zde veri depolama dÃ¼nyasÄ±nÄ±n fiili (de facto) standardÄ± ve API referans noktasÄ±dÄ±r.

### 2. Temel AmaÃ§
Her tÃ¼rlÃ¼ veriyi (yapÄ±sal veya yapÄ±sal olmayan), istenilen miktarda, %99.999999999 (11 adet 9) dayanÄ±klÄ±lÄ±kla saklamak ve internet Ã¼zerinden eriÅŸilebilir kÄ±lmaktÄ±r. Klasik dosya sistemlerinin (hiyerarÅŸik klasÃ¶r yapÄ±sÄ±) aksine, veriyi dÃ¼z (flat) bir adresleme uzayÄ±nda tutarak sonsuz Ã¶lÃ§eklenebilirlik saÄŸlar.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
S3, verileri "Bucket" (kova) adÄ± verilen kaplarda "Object" (nesne) olarak saklar.
* **Key-Value YapÄ±sÄ±:** Her dosyanÄ±n benzersiz bir anahtarÄ± (URL) vardÄ±r.
* **DÃ¼z YapÄ±:** GerÃ§ekte klasÃ¶rler yoktur, sadece isimlendirme (prefix) mantÄ±ÄŸÄ± ile klasÃ¶r varmÄ±ÅŸ gibi davranÄ±r.
* **Replikasyon:** Veri, bir bÃ¶lgedeki fiziksel olarak ayrÄ± en az 3 farklÄ± veri merkezine otomatik kopyalanÄ±r.

### 4. Temel BileÅŸenler
* **Bucket:** Nesnelerin tutulduÄŸu en Ã¼st dÃ¼zey kapsayÄ±cÄ±.
* **Object:** Verinin kendisi (Dosya) + Metadata (Veri hakkÄ±nda bilgi) + Key (Kimlik).
* **Storage Classes:** Verinin eriÅŸim sÄ±klÄ±ÄŸÄ±na gÃ¶re maliyet optimizasyonu saÄŸlayan sÄ±nÄ±flar (Standard, Glacier).

### 5. KullanÄ±m AlanlarÄ±
Data Lake (Veri GÃ¶lÃ¼), Yedekleme ve ArÅŸiv, Statik Web Sitesi sunumu.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** SÄ±nÄ±rsÄ±z Ã–lÃ§ek, YÃ¼ksek DayanÄ±klÄ±lÄ±k, Kolay Entegrasyon.
* **Dezavantajlar:** Gecikme (Yerel disklere gÃ¶re yavaÅŸtÄ±r), Egress Maliyeti (Veri indirme Ã¼creti)."""
            },
            "link": "https://aws.amazon.com/s3/", "modern": True, "dep": None,
            "code": """import boto3

# S3 Ä°stemcisi
s3 = boto3.client('s3')

# Bucket OluÅŸturma
s3.create_bucket(Bucket='my-data-lake')

# Dosya YÃ¼kleme
s3.upload_file('local_data.csv', 'my-data-lake', 'raw/data.csv')
print("Dosya yÃ¼klendi.")"""
        },
        "HDFS": {
            "desc": {"en": "Hadoop Distributed File System.", "tr": "Hadoop DaÄŸÄ±tÄ±k Dosya Sistemi."},
            "detail": {
                "en": """### 1. Definition
HDFS is a block-based, distributed file system designed to store very large files reliably across clusters of commodity hardware. Inspired by Google's GFS paper.

### 2. Core Purpose
Assuming hardware failure is the "norm" rather than exception, it splits data into chunks and distributes them across servers to ensure high throughput. Fits the "Write Once, Read Many" model.

### 3. Architecture and Working Principle
Uses a Master/Slave architecture. Files are split into blocks (Default 128 MB).
Each block is distributed to different servers (typically 3 copies). When a server fails, the system automatically uses other copies to recover data.

### 4. Key Components
* **NameNode (Master):** The "brain" of the file system. Stores metadata (file locations) in RAM.
* **DataNode (Slave):** Worker servers storing actual data blocks on disk.
* **Secondary NameNode:** Helper that merges metadata updates (checkpoints), not a backup.

### 5. Use Cases
Historical big data analysis (Batch Processing), Primary storage for engines like Spark/Hive, On-premise Data Lakes.

### 6. Pros and Cons
* **Pros:** Cost-effective (Commodity hardware), Data Locality (Compute moves to data), High Throughput.
* **Cons:** Small File Problem (NameNode memory bottleneck), Single Point of Failure (NameNode), Append-only (Files cannot be updated).""",

                "tr": """### 1. TanÄ±m
HDFS; standart (commodity) donanÄ±mlardan oluÅŸan kÃ¼meler Ã¼zerinde Ã§ok bÃ¼yÃ¼k dosyalarÄ± gÃ¼venilir bir ÅŸekilde saklamak iÃ§in tasarlanmÄ±ÅŸ, blok tabanlÄ±, daÄŸÄ±tÄ±k bir dosya sistemidir. Google'Ä±n GFS makalesinden esinlenerek geliÅŸtirilmiÅŸtir.

### 2. Temel AmaÃ§
DonanÄ±m arÄ±zalarÄ±nÄ±n "istisna" deÄŸil "kural" olduÄŸu varsayÄ±mÄ±yla, veriyi parÃ§alara bÃ¶lÃ¼p farklÄ± sunuculara daÄŸÄ±tarak yÃ¼ksek iÅŸlem hacmi (throughput) saÄŸlamaktÄ±r. "Yaz bir kere, oku Ã§ok kere" modeline uygundur.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Master/Slave mimarisi kullanÄ±r. Dosyalar belirli boyutlardaki bloklara (VarsayÄ±lan 128 MB) bÃ¶lÃ¼nÃ¼r. Her blok, kÃ¼medeki farklÄ± sunuculara (genellikle 3 kopya olarak) daÄŸÄ±tÄ±lÄ±r.

### 4. Temel BileÅŸenler
* **NameNode (Master):** Dosya sisteminin "beyni"dir. Metadata bilgisini RAM'de tutar.
* **DataNode (Slave):** Veri bloklarÄ±nÄ± fiziksel olarak diskte saklayan iÅŸÃ§i sunuculardÄ±r.
* **Secondary NameNode:** Metadata gÃ¼ncellemelerini birleÅŸtiren yardÄ±mcÄ±dÄ±r.

### 5. KullanÄ±m AlanlarÄ±
Tarihsel bÃ¼yÃ¼k veri analizi (Batch), Spark/Hive iÃ§in depolama katmanÄ±, On-premise veri gÃ¶lleri.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Maliyet (Ucuz donanÄ±m), Veri YerelliÄŸi (Data Locality), YÃ¼ksek Throughput.
* **Dezavantajlar:** KÃ¼Ã§Ã¼k Dosya Sorunu (NameNode ÅŸiÅŸer), Tek Nokta HatasÄ± (SPOF), GÃ¼ncelleme ZorluÄŸu (Sadece ekleme yapÄ±labilir)."""
            },
            "link": "https://hadoop.apache.org/", "modern": False, "dep": "NameNode",
            "code": """# HDFS CLI KomutlarÄ±

# KlasÃ¶r OluÅŸturma
hdfs dfs -mkdir /user/data

# Dosya YÃ¼kleme
hdfs dfs -put local_file.txt /user/data/

# Dosya Listeleme
hdfs dfs -ls /user/data/

# Dosya Ä°Ã§eriÄŸi Okuma
hdfs dfs -cat /user/data/local_file.txt"""
        },
        "MinIO": {
            "desc": {"en": "High Performance Object Storage.", "tr": "YÃ¼ksek PerformanslÄ± Nesne Depolama."},
            "detail": {
                "en": """### 1. Definition
MinIO is a high-performance, open-source object storage server built for Kubernetes, 100% compatible with Amazon S3 API.

### 2. Core Purpose
To provide the S3 experience and API standard with much higher performance on-premise or in private clouds. Optimized for AI/ML and analytics workloads.

### 3. Architecture and Working Principle
Written in Go, very lightweight. No complex NameNode/DataNode separation; symmetric nodes.
* **Erasure Coding:** Uses math algorithms instead of replication (3 copies) to protect data, saving up to 50% disk space.
* **Bitrot Protection:** Prevents silent data corruption over time.

### 4. Key Components
* **MinIO Server:** Runs as a single binary.
* **MinIO Client (mc):** Powerful CLI tool similar to Unix commands.
* **Console:** Web-based management UI.

### 5. Use Cases
Private Cloud (S3 alternative for regulated industries), High Performance AI/ML (Feeding GPUs), Edge Computing.

### 6. Pros and Cons
* **Pros:** Speed (Claims to be world's fastest), Simplicity (Single binary), S3 Compatibility.
* **Cons:** Management (Erasure coding complexity), Scope (Object storage only).""",

                "tr": """### 1. TanÄ±m
MinIO; Kubernetes iÃ§in oluÅŸturulmuÅŸ, Amazon S3 API ile %100 uyumlu, yÃ¼ksek performanslÄ±, aÃ§Ä±k kaynaklÄ± bir nesne depolama (Object Storage) sunucusudur.

### 2. Temel AmaÃ§
Åirketlerin kendi veri merkezlerinde (On-premise), Amazon S3'Ã¼n sunduÄŸu deneyimi ve API standardÄ±nÄ± Ã§ok daha yÃ¼ksek performansla sunmaktÄ±r. Ã–zellikle AI/ML iÅŸ yÃ¼kleri iÃ§in optimize edilmiÅŸtir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Go dili ile yazÄ±lmÄ±ÅŸtÄ±r, Ã§ok hafiftir. Simetrik dÃ¼ÄŸÃ¼m yapÄ±sÄ± kullanÄ±r.
* **Erasure Coding:** Veriyi korumak iÃ§in klasik replikasyon yerine matematiksel algoritmalar kullanÄ±r. %50 disk tasarrufu saÄŸlar.
* **Bitrot Protection:** Verinin zamanla bozulmasÄ±nÄ± engeller.

### 4. Temel BileÅŸenler
MinIO Server (Tek binary), MinIO Client (mc), Console (Web ArayÃ¼zÃ¼).

### 5. KullanÄ±m AlanlarÄ±
Ã–zel Bulut (Private Cloud), YÃ¼ksek PerformanslÄ± AI/ML, Edge Computing.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** HÄ±z, Sadelik, S3 UyumluluÄŸu.
* **Dezavantajlar:** YÃ¶netim (DaÄŸÄ±tÄ±k yapÄ± uzmanlÄ±k ister), SÄ±nÄ±rlÄ± Kapsam (Sadece Object Storage)."""
            },
            "link": "https://min.io/", "modern": True, "dep": None,
            "code": """# Docker ile MinIO Ã‡alÄ±ÅŸtÄ±rma
docker run -p 9000:9000 -p 9001:9001 \\
  minio/minio server /data --console-address ":9001"

# Python ile EriÅŸim (boto3 kullanÄ±lÄ±r)
s3 = boto3.client('s3',
    endpoint_url='http://localhost:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin')"""
        },
        "Ceph": {
            "desc": {"en": "Unified Storage Platform.", "tr": "BirleÅŸik Depolama Platformu."},
            "detail": {
                "en": """### 1. Definition
Ceph is a unified, open-source storage platform offering Object, Block, and File storage on a single distributed cluster.

### 2. Core Purpose
"One system to store it all" vision; offering a self-managing and self-healing structure scaling to Exabytes without a single point of failure (SPOF).

### 3. Architecture and Working Principle
Heart is RADOS (Reliable Autonomic Distributed Object Store).
* **CRUSH Algorithm:** Ceph's key innovation. Calculates data location algorithmically instead of using a central lookup table (like NameNode), eliminating bottlenecks.

### 4. Key Components
* **OSD:** Daemon storing data on disk.
* **MON:** Monitor maintaining cluster map.
* **MGR:** Manager for metrics.
* **Interfaces:** RadosGW (S3), RBD (Block), CephFS (File).

### 5. Use Cases
OpenStack & Kubernetes persistent storage, Academic Research (CERN), General Purpose Storage.

### 6. Pros and Cons
* **Pros:** Flexibility (Object+Block+File), Decentralization (No bottleneck), Self-Healing.
* **Cons:** Complexity (Steep learning curve), Hardware hungry.""",

                "tr": """### 1. TanÄ±m
Ceph; tek bir daÄŸÄ±tÄ±k kÃ¼me Ã¼zerinde Nesne (Object), Blok (Block) ve Dosya (File) depolamayÄ± aynÄ± anda sunan, birleÅŸik (unified), aÃ§Ä±k kaynaklÄ± bir depolama platformudur.

### 2. Temel AmaÃ§
"Her ÅŸeyi saklayabilen tek bir sistem" vizyonuyla; tek bir hata noktasÄ± (SPOF) olmadan, Exabyte seviyesine kadar Ã¶lÃ§eklenebilen, kendi kendini yÃ¶netebilen (self-managing) bir yapÄ± sunmaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Ceph'in kalbinde RADOS bulunur.
* **CRUSH AlgoritmasÄ±:** Merkezi bir tablo (NameNode gibi) kullanmaz. Verinin adresini matematiksel olarak hesaplar. Bu sayede merkezi darboÄŸaz ortadan kalkar.

### 4. Temel BileÅŸenler
OSD (Disk sÃ¼reci), MON (Ä°zleme), MGR (YÃ¶netim). ArayÃ¼zler: RadosGW (S3), RBD (Blok), CephFS (Dosya).

### 5. KullanÄ±m AlanlarÄ±
OpenStack & Kubernetes depolama, Akademik AraÅŸtÄ±rma, Genel AmaÃ§lÄ± Depolama.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Esneklik (3'Ã¼ 1 arada), Merkeziyetsizlik, Self-Healing.
* **Dezavantajlar:** KarmaÅŸÄ±klÄ±k (Ã–ÄŸrenme eÄŸrisi diktir), DonanÄ±m Ä°ÅŸtahÄ±."""
            },
            "link": "https://ceph.io/", "modern": True, "dep": "Linux",
            "code": """# Ceph KÃ¼me Durumu KontrolÃ¼
ceph status

# Yeni bir OSD (Disk) Ekleme
ceph-volume lvm create --data /dev/sdb

# Havuz (Pool) OluÅŸturma
ceph osd pool create mypool 128"""
        },
        "Ozone": {
            "desc": {"en": "Scalable Object Store.", "tr": "Ã–lÃ§eklenebilir Nesne Depolama."},
            "detail": {
                "en": """### 1. Definition
Apache Ozone is a scalable, redundant, and distributed object store for Hadoop.

### 2. Core Purpose
Designed to overcome HDFS limitations regarding small files (billions of objects).

### 3. Architecture
Separates namespace management from block management, allowing it to scale significantly better than HDFS.""",
                "tr": """### 1. TanÄ±m
Apache Ozone, Hadoop iÃ§in geliÅŸtirilmiÅŸ Ã¶lÃ§eklenebilir, daÄŸÄ±tÄ±k bir nesne depolama sistemidir.

### 2. Temel AmaÃ§
HDFS'in milyarlarca kÃ¼Ã§Ã¼k dosyayÄ± yÃ¶netememe (Small Files Problem) sorununu Ã§Ã¶zmek iÃ§in tasarlanmÄ±ÅŸtÄ±r.

### 3. Mimari
Ä°sim uzayÄ± (Namespace) yÃ¶netimi ile blok yÃ¶netimini birbirinden ayÄ±rarak HDFS'ten Ã§ok daha fazla Ã¶lÃ§eklenebilir."""
            },
            "link": "https://ozone.apache.org/", "modern": True, "dep": None,
            "code": "ozone sh volume create /vol1"
        },
        "ADLS Gen2": {
             "desc": {"en": "Azure Data Lake.", "tr": "Azure Veri GÃ¶lÃ¼."},
             "detail": {
                "en": """### 1. Definition
Microsoft Azure's enterprise data lake solution.

### 2. Core Purpose
Combines the low cost of Blob Storage with a Hierarchical File System structure optimized for analytics.""",
                "tr": """### 1. TanÄ±m
Microsoft Azure'un kurumsal veri gÃ¶lÃ¼ Ã§Ã¶zÃ¼mÃ¼dÃ¼r.

### 2. Temel AmaÃ§
Blob Storage'Ä±n ucuzluÄŸunu, analitik iÅŸlemler iÃ§in optimize edilmiÅŸ HiyerarÅŸik Dosya Sistemi yapÄ±sÄ±yla birleÅŸtirir."""
             },
             "link": "https://azure.microsoft.com/", "modern": True, "dep": None,
             "code": "# Azure SDK kullanÄ±mÄ± gerektirir"
        },
        "GCS": {
            "desc": {"en": "Google Storage.", "tr": "Google Depolama."},
            "detail": {
                "en": """### 1. Definition
Google Cloud's unified object storage service.

### 2. Core Purpose
To provide consistent, scalable storage. Famous for its strong consistency model.""",
                "tr": """### 1. TanÄ±m
Google Cloud'un birleÅŸik nesne depolama servisidir.

### 2. Temel AmaÃ§
TutarlÄ± ve Ã¶lÃ§eklenebilir depolama saÄŸlamak. GÃ¼Ã§lÃ¼ TutarlÄ±lÄ±k (Strong Consistency) modeli ile Ã¼nlÃ¼dÃ¼r."""
            },
            "link": "https://cloud.google.com/storage", "modern": True, "dep": None,
            "code": "# Google Cloud SDK kullanÄ±mÄ± gerektirir"
        }
    },
"Processing": {
        "Spark": {
            "desc": {"en": "Unified Analytics Engine.", "tr": "BirleÅŸik BÃ¼yÃ¼k Veri Motoru."},
            "detail": {
                "en": """### 1. Definition
Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters. It is the de facto standard for big data processing.

### 2. Core Purpose
To overcome the limitations of the MapReduce model (disk I/O latency) by processing data in-memory. It unifies batch, streaming, SQL, and ML workloads into a single platform.

### 3. Architecture and Working Principle
* **RDD (Resilient Distributed Dataset):** The fundamental data structure. Immutable, distributed collections of objects.
* **DAG (Directed Acyclic Graph):** Spark builds an execution plan (DAG) and optimizes it using the Catalyst Optimizer.
* **Lazy Evaluation:** Transformations are not executed until an Action (like count, save) is called.

### 4. Key Components
Spark Core, Spark SQL, Spark Streaming (Micro-batch), MLlib (Machine Learning), GraphX.

### 5. Use Cases
ETL pipelines, Exploratory Data Analysis (EDA), Machine Learning model training, Real-time dashboards.

### 6. Pros and Cons
* **Pros:** Speed (100x faster than MapReduce), Unified stack, Ease of use (Python/SQL support).
* **Cons:** Memory hungry (OOM errors are common), Complexity in tuning.""",

                "tr": """### 1. TanÄ±m
Apache Spark; veri mÃ¼hendisliÄŸi, veri bilimi ve makine Ã¶ÄŸrenimi iÅŸ yÃ¼klerini tek dÃ¼ÄŸÃ¼mlÃ¼ makinelerde veya kÃ¼melerde yÃ¼rÃ¼tmek iÃ§in tasarlanmÄ±ÅŸ Ã§ok dilli bir motordur. BÃ¼yÃ¼k veri iÅŸlemenin fiili standardÄ±dÄ±r.

### 2. Temel AmaÃ§
MapReduce modelinin disk G/Ã‡ gecikmelerinden kaynaklanan yavaÅŸlÄ±ÄŸÄ±nÄ±, veriyi bellek iÃ§inde (In-Memory) iÅŸleyerek aÅŸmaktÄ±r. Toplu (Batch), AkÄ±ÅŸ (Stream), SQL ve Makine Ã–ÄŸrenmesi iÅŸ yÃ¼klerini tek platformda birleÅŸtirir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
* **RDD:** Temel veri yapÄ±sÄ±dÄ±r. DeÄŸiÅŸtirilemez ve daÄŸÄ±tÄ±k nesne koleksiyonudur.
* **DAG:** Spark, yapÄ±lacak iÅŸlemleri bir Ã§izge (grafik) olarak planlar ve Catalyst Optimizer ile optimize eder.
* **Tembel DeÄŸerlendirme (Lazy Evaluation):** Bir "Eylem" (Action - Ã¶rn: kaydet, say) Ã§aÄŸrÄ±lana kadar hiÃ§bir iÅŸlem Ã§alÄ±ÅŸtÄ±rÄ±lmaz, sadece planlanÄ±r.

### 4. Temel BileÅŸenler
Spark Core, Spark SQL, Spark Streaming (Mikro-yÄ±ÄŸÄ±n), MLlib, GraphX.

### 5. KullanÄ±m AlanlarÄ±
ETL boru hatlarÄ±, KeÅŸifsel Veri Analizi, ML model eÄŸitimi, GerÃ§ek zamanlÄ± paneller.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** HÄ±z (MapReduce'tan 100 kat hÄ±zlÄ±), BirleÅŸik yapÄ±, KullanÄ±m kolaylÄ±ÄŸÄ± (Python/SQL).
* **Dezavantajlar:** Bellek canavarÄ±dÄ±r (RAM yetmezliÄŸi hatalarÄ± sÄ±ktÄ±r), Ä°nce ayar (Tuning) uzmanlÄ±k ister."""
            },
            "link": "https://spark.apache.org/", "modern": True, "dep": "Cluster",
            "code": """from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Demo").getOrCreate()

# Veri Okuma
df = spark.read.csv("sales.csv", header=True)

# Ä°ÅŸleme (Transformation)
result = df.groupBy("category").count()

# SonuÃ§ (Action)
result.show()"""
        },
        "Flink": {
            "desc": {"en": "Stateful Stream Processing.", "tr": "Durumlu AkÄ±ÅŸ Ä°ÅŸleme."},
            "detail": {
                "en": """### 1. Definition
Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams.

### 2. Core Purpose
To process continuous data streams in real-time with ultra-low latency and guarantees of correctness (Exactly-once semantics), unlike Spark's micro-batch approach.

### 3. Architecture and Working Principle
Flink processes data row-by-row as it arrives.
* **Event Time:** Handles data based on when it occurred, not when it arrived.
* **Watermarks:** A mechanism to handle late-arriving data.
* **State Backends:** Stores intermediate state (e.g., in RocksDB) for fault tolerance.

### 4. Key Components
DataStream API, Table API/SQL, Flink CEP (Complex Event Processing).

### 5. Use Cases
Fraud detection, Real-time recommendation, Network monitoring.

### 6. Pros and Cons
* **Pros:** True streaming (Low latency), Powerful state management, Exactly-once guarantee.
* **Cons:** Operational complexity, Steeper learning curve than Spark.""",

                "tr": """### 1. TanÄ±m
Apache Flink; sÄ±nÄ±rsÄ±z ve sÄ±nÄ±rlÄ± veri akÄ±ÅŸlarÄ± Ã¼zerinde durumlu (stateful) hesaplamalar yapmak iÃ§in geliÅŸtirilmiÅŸ daÄŸÄ±tÄ±k bir iÅŸlem motorudur.

### 2. Temel AmaÃ§
Spark'Ä±n mikro-yÄ±ÄŸÄ±n yaklaÅŸÄ±mÄ±nÄ±n aksine, sÃ¼rekli veri akÄ±ÅŸlarÄ±nÄ± gerÃ§ek zamanlÄ± olarak, ultra dÃ¼ÅŸÃ¼k gecikmeyle ve doÄŸruluk garantisiyle (Tam bir kez iÅŸleme) iÅŸlemektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Flink veriyi geldiÄŸi an satÄ±r satÄ±r iÅŸler.
* **Olay ZamanÄ± (Event Time):** Veriyi sunucuya varÄ±ÅŸ zamanÄ±na gÃ¶re deÄŸil, oluÅŸtuÄŸu zamana gÃ¶re iÅŸler.
* **Watermarks:** GeÃ§ gelen verileri yÃ¶netmek iÃ§in kullanÄ±lan zaman iÅŸaretÃ§ileridir.
* **State Backends:** Hata toleransÄ± iÃ§in ara durumlarÄ± (State) RocksDB gibi yerlerde saklar.

### 4. Temel BileÅŸenler
DataStream API, Table API/SQL, Flink CEP.

### 5. KullanÄ±m AlanlarÄ±
DolandÄ±rÄ±cÄ±lÄ±k tespiti, GerÃ§ek zamanlÄ± Ã¶neri, AÄŸ izleme.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** GerÃ§ek akÄ±ÅŸ (DÃ¼ÅŸÃ¼k gecikme), GÃ¼Ã§lÃ¼ durum yÃ¶netimi, Exactly-once garantisi.
* **Dezavantajlar:** Operasyonel karmaÅŸÄ±klÄ±k, Ã–ÄŸrenme eÄŸrisi Spark'tan diktir."""
            },
            "link": "https://flink.apache.org/", "modern": True, "dep": "Zookeeper",
            "code": """// Java API
DataStream<String> stream = env.socketTextStream("localhost", 9999);

stream.flatMap(new Tokenizer())
      .keyBy(value -> value.f0)
      .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
      .sum(1)
      .print();"""
        },
        "Trino": {
            "desc": {"en": "Distributed SQL Query Engine.", "tr": "Federatif SQL Motoru."},
            "detail": {
                "en": """### 1. Definition
Trino (formerly PrestoSQL) is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources.

### 2. Core Purpose
To separate Compute from Storage. It allows querying data where it lives (S3, Kafka, MySQL, Cassandra) without moving/copying it to a central warehouse.

### 3. Architecture and Working Principle
MPP (Massively Parallel Processing) architecture.
* **Coordinator:** Parses SQL, plans query, manages workers.
* **Workers:** Execute tasks and fetch data from connectors.
* **Connectors:** Adapters for different data sources.

### 4. Key Components
Coordinator, Workers, Connectors.

### 5. Use Cases
Ad-hoc analytics on Data Lakes, Data Federation (Joining Kafka stream with MySQL table).

### 6. Pros and Cons
* **Pros:** Fast interactive queries, No data movement needed, ANSI SQL support.
* **Cons:** Not for OLTP (transactions), Resource intensive in memory.""",

                "tr": """### 1. TanÄ±m
Trino (eski adÄ±yla PrestoSQL); farklÄ± kaynaklara daÄŸÄ±lmÄ±ÅŸ bÃ¼yÃ¼k veri setlerini sorgulamak iÃ§in tasarlanmÄ±ÅŸ daÄŸÄ±tÄ±k bir SQL motorudur.

### 2. Temel AmaÃ§
Hesaplama (Compute) ile DepolamayÄ± (Storage) ayÄ±rmaktÄ±r. Veriyi merkezi bir ambara kopyalamadan, olduÄŸu yerde (S3, Kafka, MySQL) sorgulamayÄ± saÄŸlar.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
MPP (Devasa Paralel Ä°ÅŸleme) mimarisini kullanÄ±r.
* **KoordinatÃ¶r:** SQL'i parÃ§alar, planlar ve iÅŸÃ§ileri yÃ¶netir.
* **Ä°ÅŸÃ§iler (Workers):** Veriyi kaynaklardan Ã§eker ve iÅŸler.
* **KonektÃ¶rler:** FarklÄ± veri kaynaklarÄ±na (Hive, Postgres) baÄŸlanmayÄ± saÄŸlayan adaptÃ¶rler.

### 4. Temel BileÅŸenler
KoordinatÃ¶r, Ä°ÅŸÃ§iler, KonektÃ¶rler.

### 5. KullanÄ±m AlanlarÄ±
Veri gÃ¶lÃ¼ Ã¼zerinde anlÄ±k analiz, Veri Federasyonu (Kafka akÄ±ÅŸÄ±nÄ± MySQL tablosuyla birleÅŸtirme).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** HÄ±zlÄ± etkileÅŸimli sorgular, Veri taÅŸÄ±ma gerektirmez, Standart SQL.
* **Dezavantajlar:** Ä°ÅŸlem (Transaction) veritabanÄ± deÄŸildir, Bellek kullanÄ±mÄ± yÃ¼ksektir."""
            },
            "link": "https://trino.io/", "modern": True, "dep": "Catalog",
            "code": """-- S3'teki veriyi MySQL ile birleÅŸtirme
SELECT 
    u.name, 
    o.amount 
FROM 
    mysql.crm.users u
JOIN 
    hive.sales.orders o 
ON u.id = o.user_id;"""
        },
        "Storm": {
            "desc": {"en": "Real-time Computation.", "tr": "Eski Nesil AkÄ±ÅŸ Ä°ÅŸleme."},
            "detail": {
                "en": """### 1. Definition
Apache Storm is a distributed realtime computation system. It pioneered the processing of unbounded streams of data.

### 2. Core Purpose
To provide a simple and robust way to process real-time data streams, similar to how Hadoop processes batch data.

### 3. Architecture and Working Principle
* **Topology:** A graph of computation. Runs forever until killed.
* **Spout:** Source of data streams.
* **Bolt:** Processes input streams and produces new streams.

### 4. Key Components
Nimbus (Master), Supervisor (Worker), Zookeeper.

### 5. Use Cases
Real-time analytics, Online machine learning. (Note: Mostly replaced by Flink/Spark Streaming).

### 6. Pros and Cons
* **Pros:** Extremely low latency, Simple programming model.
* **Cons:** Lacks "Exactly-once" processing guarantees (only At-least-once), Managing state is hard.""",

                "tr": """### 1. TanÄ±m
Apache Storm, daÄŸÄ±tÄ±k bir gerÃ§ek zamanlÄ± hesaplama sistemidir. SÄ±nÄ±rsÄ±z veri akÄ±ÅŸlarÄ±nÄ± iÅŸlemenin Ã¶ncÃ¼sÃ¼dÃ¼r.

### 2. Temel AmaÃ§
Hadoop'un toplu veriler iÃ§in yaptÄ±ÄŸÄ±nÄ±, gerÃ§ek zamanlÄ± veriler iÃ§in yapmaktÄ±r: Basit ve gÃ¼venilir bir iÅŸlem Ã§erÃ§evesi sunmak.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
* **Topoloji:** Ä°ÅŸlem grafiÄŸidir. Durdurulana kadar sonsuza dek Ã§alÄ±ÅŸÄ±r.
* **Spout:** Veri kaynaÄŸÄ± (Musluk).
* **Bolt:** Veriyi iÅŸleyen ve dÃ¶nÃ¼ÅŸtÃ¼ren birim (CÄ±vata).

### 4. Temel BileÅŸenler
Nimbus (YÃ¶netici), Supervisor (Ä°ÅŸÃ§i), Zookeeper.

### 5. KullanÄ±m AlanlarÄ±
GerÃ§ek zamanlÄ± analitik. (Not: GÃ¼nÃ¼mÃ¼zde yerini Flink ve Spark Streaming'e bÄ±rakmÄ±ÅŸtÄ±r).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Ã‡ok dÃ¼ÅŸÃ¼k gecikme, Basit model.
* **Dezavantajlar:** "Tam bir kez" (Exactly-once) iÅŸleme garantisi yoktur, Durum (State) yÃ¶netimi zordur."""
            },
            "link": "https://storm.apache.org/", "modern": False, "dep": "Zookeeper",
            "code": """// Java Topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("words", new TestWordSpout(), 10);
builder.setBolt("exclaim", new ExclamationBolt(), 3).shuffleGrouping("words");"""
        },
        "Beam": {
            "desc": {"en": "Unified Programming Model.", "tr": "BirleÅŸik Programlama Modeli."},
            "detail": {
                "en": """### 1. Definition
Apache Beam is an advanced unified programming model for defining both batch and streaming data-parallel processing pipelines.

### 2. Core Purpose
"Write once, run anywhere." To decouple the pipeline logic from the execution engine. You write code in Beam, and it runs on Spark, Flink, or Google Dataflow.

### 3. Architecture and Working Principle
* **Pipeline:** The entire data processing task.
* **PCollection:** A dataset (bounded or unbounded).
* **PTransform:** A data processing operation.
* **Runner:** The backend that executes the pipeline (e.g., SparkRunner).

### 4. Key Components
SDKs (Java, Python, Go), Runners.

### 5. Use Cases
Building portable data pipelines, migrating between cloud providers.

### 6. Pros and Cons
* **Pros:** Portability, Unified API for Batch/Stream.
* **Cons:** Debugging can be complex (abstraction layer), Performance overhead compared to native APIs.""",

                "tr": """### 1. TanÄ±m
Apache Beam; hem toplu (batch) hem de akÄ±ÅŸ (stream) veri iÅŸleme hatlarÄ±nÄ± tanÄ±mlamak iÃ§in geliÅŸtirilmiÅŸ birleÅŸik bir programlama modelidir.

### 2. Temel AmaÃ§
"Bir kez yaz, her yerde Ã§alÄ±ÅŸtÄ±r." Veri iÅŸleme mantÄ±ÄŸÄ±nÄ±, Ã§alÄ±ÅŸtÄ±rma motorundan ayÄ±rmaktÄ±r. Kodu Beam ile yazarsÄ±nÄ±z; Spark, Flink veya Google Dataflow Ã¼zerinde Ã§alÄ±ÅŸÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
* **Pipeline:** TÃ¼m veri iÅŸleme gÃ¶revi.
* **PCollection:** Veri seti (SÄ±nÄ±rlÄ± veya sÄ±nÄ±rsÄ±z).
* **PTransform:** Veri iÅŸleme operasyonu (Map, Filter).
* **Runner:** Kodu Ã§alÄ±ÅŸtÄ±ran motor (Ã–rn: SparkRunner).

### 4. Temel BileÅŸenler
SDK'lar (Java, Python), Runner'lar.

### 5. KullanÄ±m AlanlarÄ±
TaÅŸÄ±nabilir veri boru hatlarÄ±, Bulut saÄŸlayÄ±cÄ±larÄ± arasÄ± geÃ§iÅŸ.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** TaÅŸÄ±nabilirlik, Batch/Stream iÃ§in tek API.
* **Dezavantajlar:** Hata ayÄ±klama zordur (Soyutlama katmanÄ± yÃ¼zÃ¼nden), Yerel API'lere gÃ¶re performans kaybÄ± olabilir."""
            },
            "link": "https://beam.apache.org/", "modern": True, "dep": None,
            "code": """import apache_beam as beam

with beam.Pipeline() as p:
    (p | beam.Create(['Hello', 'World'])
       | beam.Map(print))"""
        },
        "dbt": {
            "desc": {"en": "Data Transformation Tool.", "tr": "Analitik DÃ¶nÃ¼ÅŸtÃ¼rme AracÄ±."},
            "detail": {
                "en": """### 1. Definition
dbt (data build tool) is a transformation workflow that lets analysts and engineers transform data in their warehouses by simply writing SQL.

### 2. Core Purpose
To bring software engineering best practices (version control, testing, documentation, CI/CD) to the world of data analysts. It owns the "T" in ELT.

### 3. Architecture and Working Principle
dbt compiles code into raw SQL and runs it against your database.
* **Models:** SQL files containing SELECT statements.
* **Jinja:** Templating language to write dynamic SQL (loops, variables).
* **DAG:** dbt automatically infers dependencies between models.

### 4. Key Components
Models, Tests, Seeds, Snapshots, Docs.

### 5. Use Cases
Building data marts, Metrics standardization, Data quality testing.

### 6. Pros and Cons
* **Pros:** SQL-based (Low barrier to entry), Git integration, Automated documentation.
* **Cons:** Requires a powerful Data Warehouse (Snowflake/BigQuery), not for general programming.""",

                "tr": """### 1. TanÄ±m
dbt (data build tool); analistlerin ve mÃ¼hendislerin sadece SQL yazarak veri ambarlarÄ±ndaki veriyi dÃ¶nÃ¼ÅŸtÃ¼rmelerini saÄŸlayan bir araÃ§tÄ±r.

### 2. Temel AmaÃ§
YazÄ±lÄ±m mÃ¼hendisliÄŸi prensiplerini (Versiyon kontrolÃ¼, Test, CI/CD) analitik dÃ¼nyasÄ±na getirmektir. ELT sÃ¼recindeki 'T' (Transformation) harfini sahiplenir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
dbt, yazdÄ±ÄŸÄ±nÄ±z kodu ham SQL'e derler ve veritabanÄ±nda Ã§alÄ±ÅŸtÄ±rÄ±r.
* **Modeller:** SELECT sorgularÄ± iÃ§eren SQL dosyalarÄ±.
* **Jinja:** Dinamik SQL yazmak iÃ§in ÅŸablonlama dili.
* **DAG:** Modeller arasÄ±ndaki baÄŸÄ±mlÄ±lÄ±ÄŸÄ± otomatik Ã§Ã¶zer.

### 4. Temel BileÅŸenler
Modeller, Testler, DokÃ¼mantasyon.

### 5. KullanÄ±m AlanlarÄ±
Veri pazarlarÄ± (Data Marts) oluÅŸturma, Veri kalitesi testleri.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** SQL tabanlÄ± (Kolay Ã¶ÄŸrenilir), Git entegrasyonu, Otomatik dokÃ¼man.
* **Dezavantajlar:** GÃ¼Ã§lÃ¼ bir Veri AmbarÄ± gerektirir, genel programlama iÃ§in deÄŸildir."""
            },
            "link": "https://www.getdbt.com/", "modern": True, "dep": "Warehouse",
            "code": """-- models/clean_users.sql
WITH raw_users AS (
    SELECT * FROM {{ source('raw', 'users') }}
)
SELECT 
    id, 
    lower(email) as email 
FROM raw_users"""
        },
        "Databricks": {
            "desc": {"en": "Data Intelligence Platform.", "tr": "YÃ¶netilen Veri ZekasÄ± Platformu."},
            "detail": {
                "en": """### 1. Definition
Databricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions. Founded by the creators of Apache Spark.

### 2. Core Purpose
To unify Data Warehousing and Data Lakes into a "Lakehouse" architecture. To provide a collaborative environment for Data Engineers and Data Scientists.

### 3. Architecture and Working Principle
Built on top of open standards (Spark, Delta Lake, MLflow).
* **Control Plane:** Managed by Databricks (Web UI, Notebooks, Job Scheduler).
* **Data Plane:** Your cloud account (AWS/Azure/GCP) where data is processed and stored.

### 4. Key Components
Workspace, Notebooks, Delta Lake, Unity Catalog (Governance).

### 5. Use Cases
Lakehouse implementation, MLOps, Collaborative Data Science.

### 6. Pros and Cons
* **Pros:** Best Spark experience, Unified platform, Lakehouse pioneer.
* **Cons:** Cost (Can be expensive), Complexity for simple tasks.""",

                "tr": """### 1. TanÄ±m
Databricks; kurumsal veri, analitik ve yapay zeka Ã§Ã¶zÃ¼mleri geliÅŸtirmek iÃ§in kullanÄ±lan birleÅŸik bir platformdur. Apache Spark'Ä±n yaratÄ±cÄ±larÄ± tarafÄ±ndan kurulmuÅŸtur.

### 2. Temel AmaÃ§
Veri AmbarlarÄ± ile Veri GÃ¶llerini "Lakehouse" mimarisinde birleÅŸtirmek. Veri MÃ¼hendisleri ve Veri Bilimciler iÃ§in ortak Ã§alÄ±ÅŸma alanÄ± sunmak.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
AÃ§Ä±k standartlar Ã¼zerine kuruludur (Spark, Delta Lake).
* **Kontrol DÃ¼zlemi:** Databricks tarafÄ±ndan yÃ¶netilen arayÃ¼z ve notebook'lar.
* **Veri DÃ¼zlemi:** Verinin iÅŸlendiÄŸi ve saklandÄ±ÄŸÄ± sizin bulut hesabÄ±nÄ±z.

### 4. Temel BileÅŸenler
Workspace, Notebooks, Delta Lake, Unity Catalog.

### 5. KullanÄ±m AlanlarÄ±
Lakehouse kurulumu, MLOps, Ortak Veri Bilimi projeleri.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** En iyi Spark deneyimi, BirleÅŸik platform, Lakehouse Ã¶ncÃ¼sÃ¼.
* **Dezavantajlar:** Maliyet (PahalÄ± olabilir), Basit iÅŸler iÃ§in karmaÅŸÄ±k kaÃ§abilir."""
            },
            "link": "https://www.databricks.com/", "modern": True, "dep": "Cloud",
            "code": """# Databricks Notebook Kodu
# Delta Tablosu okuma
df = spark.read.format("delta").load("/mnt/delta/events")
display(df)"""
        },
        "Hadoop MR": {
            "desc": {"en": "Legacy Batch Processing.", "tr": "Eski Nesil Toplu Ä°ÅŸleme."},
            "detail": {
                "en": """### 1. Definition
MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.

### 2. Core Purpose
To allow processing of data that is too large to fit into a single machine's memory by leveraging disk storage and processing power of multiple machines.

### 3. Architecture and Working Principle
* **Map Phase:** Filters and sorts data.
* **Shuffle & Sort:** Transfers data between nodes.
* **Reduce Phase:** Aggregates data.
It writes intermediate results to disk, making it fault-tolerant but slow.

### 4. Key Components
Mapper, Reducer, JobTracker (Legacy), TaskTracker.

### 5. Use Cases
Massive batch jobs where speed is not critical (e.g., nightly indexing). (Replaced by Spark).

### 6. Pros and Cons
* **Pros:** Scalability, Fault Tolerance, Simplicity of model.
* **Cons:** High Latency (Disk I/O), Verbose code (Java), Hard to manage chain of jobs.""",

                "tr": """### 1. TanÄ±m
MapReduce; bÃ¼yÃ¼k veri setlerini paralel ve daÄŸÄ±tÄ±k bir algoritma ile iÅŸlemek iÃ§in kullanÄ±lan bir programlama modelidir.

### 2. Temel AmaÃ§
Tek bir makinenin belleÄŸine sÄ±ÄŸmayacak kadar bÃ¼yÃ¼k verileri, Ã§ok sayÄ±da makinenin disk ve iÅŸlemci gÃ¼cÃ¼nÃ¼ kullanarak iÅŸlemek.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
* **Map FazÄ±:** Veriyi filtreler ve sÄ±ralar.
* **Shuffle & Sort:** Veriyi dÃ¼ÄŸÃ¼mler arasÄ± taÅŸÄ±r.
* **Reduce FazÄ±:** Veriyi Ã¶zetler (Toplar).
Her adÄ±mda ara sonuÃ§larÄ± diske yazar, bu onu gÃ¼venilir ama yavaÅŸ yapar.

### 4. Temel BileÅŸenler
Mapper, Reducer.

### 5. KullanÄ±m AlanlarÄ±
HÄ±zÄ±n kritik olmadÄ±ÄŸÄ± devasa toplu iÅŸler. (Yerini Spark'a bÄ±rakmÄ±ÅŸtÄ±r).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Ã–lÃ§eklenebilirlik, Hata ToleransÄ±.
* **Dezavantajlar:** YÃ¼ksek Gecikme (Disk I/O), Ã‡ok kod yazma gereÄŸi (Java)."""
            },
            "link": "https://hadoop.apache.org/", "modern": False, "dep": "HDFS, YARN",
            "code": """public class WordCount {
  public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{...}
  public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable>{...}
}"""
        }
    },
"Databases": {
        "Snowflake": {
            "desc": {"en": "Cloud Data Warehouse.", "tr": "Bulut Veri AmbarÄ±."},
            "detail": {
                "en": """### 1. Definition
Snowflake is a cloud-native Modern Data Warehouse offered as SaaS, which completely separates storage and compute layers.

### 2. Core Purpose
To solve the flexibility issues of traditional data warehouses (Oracle Exadata, Teradata) and utilize the limitless resources of the cloud to automate data management (tuning, indexing) and increase concurrent query performance.

### 3. Architecture and Working Principle
Snowflake's revolutionary feature is the "Multi-Cluster Shared Data" architecture.
* **Storage:** Data is stored centrally on S3, Azure Blob, or GCS (Shared Data).
* **Compute:** Queries run on virtual clusters called "Virtual Warehouses".
* **Separation:** Storage and compute are independent. Different departments (Finance, Marketing) can work on the same data with different sized virtual machines without slowing each other down.

### 4. Key Components
* **Virtual Warehouse:** Compute power processing queries (measured in T-shirt sizes like XS, M, XL).
* **Snowpipe:** Service for continuous data ingestion.
* **Time Travel:** Ability to query data as it existed in the past (e.g., 5 hours ago).

### 5. Use Cases
Modern Data Sharing, BI Reporting (Tableau backend).

### 6. Pros and Cons
* **Pros:** Zero Management (No indexing/vacuuming), Concurrency (Scales instantly).
* **Cons:** Cost Control (Pay-as-you-go can lead to surprise bills), Vendor Lock-in (Cloud only).""",

                "tr": """### 1. TanÄ±m
Snowflake; bulut iÃ§in sÄ±fÄ±rdan tasarlanmÄ±ÅŸ (Cloud-Native), depolama ve hesaplama katmanlarÄ±nÄ± birbirinden tamamen ayÄ±ran, Hizmet Olarak YazÄ±lÄ±m (SaaS) modeliyle sunulan bir Modern Veri AmbarÄ±dÄ±r.

### 2. Temel AmaÃ§
Geleneksel veri ambarlarÄ±nÄ±n esneklik sorunlarÄ±nÄ± Ã§Ã¶zmek ve bulutun sÄ±nÄ±rsÄ±z kaynaÄŸÄ±nÄ± kullanarak; veri yÃ¶netimini otomatize etmek ve eÅŸzamanlÄ± (concurrency) sorgu performansÄ±nÄ± artÄ±rmaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Snowflake'in devrimsel Ã¶zelliÄŸi "Multi-Cluster Shared Data" mimarisidir.
* **Depolama:** Veri S3, Azure Blob veya GCS Ã¼zerinde merkezi olarak durur.
* **Hesaplama:** Sorgular "Virtual Warehouse" adÄ± verilen sanal kÃ¼melerde Ã§alÄ±ÅŸÄ±r.
* **AyrÄ±m:** Depolama ve iÅŸlemci baÄŸÄ±msÄ±zdÄ±r. Veriyi kopyalamadan, farklÄ± departmanlar aynÄ± veri Ã¼zerinde birbirini yavaÅŸlatmadan Ã§alÄ±ÅŸabilir.

### 4. Temel BileÅŸenler
Virtual Warehouse (Sanal Ambar), Snowpipe (AnlÄ±k Veri AlÄ±mÄ±), Time Travel (Zamanda Yolculuk).

### 5. KullanÄ±m AlanlarÄ±
Modern Veri PaylaÅŸÄ±mÄ±, BI Raporlama.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** SÄ±fÄ±r YÃ¶netim (BakÄ±m yok), Concurrency (YavaÅŸlama yok).
* **Dezavantajlar:** Maliyet KontrolÃ¼ (SÃ¼rpriz fatura riski), Vendor Lock-in (Sadece bulutta Ã§alÄ±ÅŸÄ±r)."""
            },
            "link": "https://www.snowflake.com/", "modern": True, "dep": None,
            "code": """-- JSON veriyi sorgulama (Variant Type)
SELECT 
    v:device_type as device,
    v:location.city as city
FROM raw_logs
WHERE v:temperature > 20;"""
        },
        "PostgreSQL": {
            "desc": {"en": "Advanced RDBMS.", "tr": "GeliÅŸmiÅŸ Ä°liÅŸkisel VeritabanÄ±."},
            "detail": {
                "en": """### 1. Definition
PostgreSQL is an open-source Object-Relational Database Management System (ORDBMS) with over 30 years of active development, strictly adhering to SQL standards.

### 2. Core Purpose
To manage complex queries and large transactions faultlessly by prioritizing Data Integrity and reliability above all else.

### 3. Architecture and Working Principle
It has a classic "Process-based" architecture.
* **MVCC (Multiversion Concurrency Control):** While one user reads data, another can update it. Readers do not wait for locks; they see the old version. This ensures high concurrency.
* **WAL (Write-Ahead Logging):** Data is written to a log before disk, preventing data loss even during power failures.

### 4. Key Components
* **Postmaster:** The main managing process.
* **Shared Buffers:** Common memory area.
* **Extensions:** Features added later (Most famous: PostGIS for geospatial data).

### 5. Use Cases
Metadata Store (Airflow, Hive metastore), OLTP Systems (E-commerce, Banking).

### 6. Pros and Cons
* **Pros:** ACID Compliance, Extensibility (JSON, Geo), Horizontal Scaling (Read Replicas).
* **Cons:** Write Scaling (Sharding is hard), Speed (Slower than Redis for simple KV lookups).""",

                "tr": """### 1. TanÄ±m
PostgreSQL; 30 yÄ±lÄ± aÅŸkÄ±n sÃ¼redir geliÅŸtirilen, SQL standartlarÄ±na sÄ±kÄ± sÄ±kÄ±ya baÄŸlÄ±, aÃ§Ä±k kaynaklÄ± Nesne-Ä°liÅŸkisel VeritabanÄ± YÃ¶netim Sistemidir (ORDBMS).

### 2. Temel AmaÃ§
Veri tutarlÄ±lÄ±ÄŸÄ±nÄ± (Data Integrity) ve gÃ¼venilirliÄŸi her ÅŸeyin Ã¼stÃ¼nde tutarak, karmaÅŸÄ±k sorgularÄ± ve bÃ¼yÃ¼k iÅŸlemleri (Transaction) hatasÄ±z yÃ¶netmektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Klasik "Process-based" mimariye sahiptir.
* **MVCC:** Bir kullanÄ±cÄ± veriyi okurken, diÄŸer kullanÄ±cÄ± gÃ¼ncelleyebilir. Okuyan kiÅŸi kilitlenmeyi beklemez, eski versiyonu gÃ¶rÃ¼r.
* **WAL:** Veri diske yazÄ±lmadan Ã¶nce gÃ¼nlÃ¼ÄŸe yazÄ±lÄ±r, veri kaybÄ±nÄ± Ã¶nler.

### 4. Temel BileÅŸenler
Postmaster (Ana SÃ¼reÃ§), Shared Buffers (Ortak Bellek), Extensions (Eklentiler - Ã¶rn: PostGIS).

### 5. KullanÄ±m AlanlarÄ±
Metadata Store (Airflow, Hive vb.), OLTP Sistemler.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** ACID Uyumu, GeniÅŸletilebilirlik (JSON, Geo), GÃ¼venilirlik.
* **Dezavantajlar:** Yazma Ã–lÃ§eklenmesi (Sharding zordur), HÄ±z (Basit okumalarda NoSQL'den yavaÅŸtÄ±r)."""
            },
            "link": "https://www.postgresql.org/", "modern": True, "dep": None,
            "code": """-- JSONB kullanÄ±mÄ±
CREATE TABLE orders (
    id serial PRIMARY KEY,
    info jsonb
);

INSERT INTO orders (info) VALUES ('{"customer": "John", "items": ["book", "pen"]}');

SELECT info->>'customer' FROM orders WHERE info->'items' ? 'book';"""
        },
        "Neo4j": {
            "desc": {"en": "Graph Database.", "tr": "Graf VeritabanÄ±."},
            "detail": {
                "en": """### 1. Definition
Neo4j is a Graph Database that treats relationships between data as first-class citizens.

### 2. Core Purpose
To performantly solve deep connection and relationship queries (like "movies my friend's friend liked") where relational databases (SQL) struggle.

### 3. Architecture and Working Principle
Stores data as "Nodes" and "Relationships" instead of tables.
* **Index-Free Adjacency:** Each node knows the physical address of the connected node. Navigating (hopping) between millions of connections takes milliseconds.

### 4. Key Components
* **Node:** Entity (e.g., John).
* **Relationship:** Connection (e.g., KNOWS).
* **Property:** Attribute (e.g., Age: 30).
* **Cypher:** SQL-like query language specific to graphs.

### 5. Use Cases
Fraud Detection, Social Networks, Recommendation Engines.

### 6. Pros and Cons
* **Pros:** Relationship Performance (No JOINs needed), Visual Mental Model.
* **Cons:** Scaling (Sharding graph data is very hard), Niche Use Case.""",

                "tr": """### 1. TanÄ±m
Neo4j; veriler arasÄ±ndaki iliÅŸkileri (relationships) birinci sÄ±nÄ±f vatandaÅŸ olarak ele alan, grafik tabanlÄ± (Graph Database) bir veritabanÄ±dÄ±r.

### 2. Temel AmaÃ§
Ä°liÅŸkisel veritabanlarÄ±nÄ±n (SQL) Ã§ok zorlandÄ±ÄŸÄ± Ã§ok derin baÄŸlantÄ± ve iliÅŸki sorgularÄ±nÄ± performanslÄ± bir ÅŸekilde Ã§Ã¶zmektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Veriyi tablolar yerine "Nodes" (DÃ¼ÄŸÃ¼mler) ve "Relationships" (Ä°liÅŸkiler) olarak saklar.
* **Index-Free Adjacency:** Her dÃ¼ÄŸÃ¼m, baÄŸlÄ± olduÄŸu diÄŸer dÃ¼ÄŸÃ¼mÃ¼n adresini bilir. Milyonlarca baÄŸlantÄ± arasÄ±nda gezinmek milisaniyeler sÃ¼rer.

### 4. Temel BileÅŸenler
Node (VarlÄ±k), Relationship (Ä°liÅŸki), Property (Ã–zellik), Cypher (Sorgu Dili).

### 5. KullanÄ±m AlanlarÄ±
DolandÄ±rÄ±cÄ±lÄ±k Tespiti, Sosyal AÄŸlar, Ã–neri MotorlarÄ±.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Ä°liÅŸki PerformansÄ± (JOIN gerektirmez), GÃ¶rsellik.
* **Dezavantajlar:** Ã–lÃ§eklenme (Sharding zordur), NiÅŸ KullanÄ±m."""
            },
            "link": "https://neo4j.com/", "modern": True, "dep": None,
            "code": """// Cypher Sorgu Dili
MATCH (user:Person)-[:FRIEND]->(friend:Person)-[:LIKES]->(movie:Movie)
WHERE user.name = 'Ahmet'
RETURN movie.title"""
        },
        "Hive": {
            "desc": {"en": "SQL on Hadoop.", "tr": "Hadoop Veri AmbarÄ±."},
            "detail": {
                "en": """### 1. Definition
Apache Hive is a data warehouse infrastructure built on top of Hadoop HDFS to query and analyze massive datasets using SQL-like language (HiveQL).

### 2. Core Purpose
To allow data analysts who do not know Java to query Petabytes of data on Hadoop using SQL, without writing complex MapReduce code.

### 3. Architecture and Working Principle
Hive is a "translator", not a database.
* User writes SQL.
* Hive translates this into MapReduce, Tez, or Spark jobs.
* Jobs run on the cluster and return results.

### 4. Key Components
* **Metastore:** Central database storing table definitions.
* **Driver:** Plans and optimizes queries.
* **Execution Engine:** The engine running the job (MapReduce, Tez, Spark).

### 5. Use Cases
ETL Processes, Batch Reporting.

### 6. Pros and Cons
* **Pros:** SQL Ease, Scalability (Petabytes).
* **Cons:** High Latency (Not real-time), Limited Transactions.""",

                "tr": """### 1. TanÄ±m
Apache Hive; Hadoop HDFS Ã¼zerinde tutulan devasa veri setlerini sorgulamak ve analiz etmek iÃ§in geliÅŸtirilmiÅŸ, SQL benzeri (HiveQL) bir veri ambarÄ± altyapÄ±sÄ±dÄ±r.

### 2. Temel AmaÃ§
Java bilmeyen veri analistlerinin, karmaÅŸÄ±k MapReduce kodlarÄ± yazmadan, bildikleri SQL dilini kullanarak Hadoop Ã¼zerindeki veriyi sorgulayabilmesini saÄŸlamaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Hive bir veritabanÄ± deÄŸil, bir "Ã§evirmen"dir. KullanÄ±cÄ± SQL yazar, Hive bunu arka planda MapReduce veya Spark iÅŸlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

### 4. Temel BileÅŸenler
Metastore (Tablo tanÄ±mlarÄ±), Driver (PlanlayÄ±cÄ±), Execution Engine (Ä°ÅŸlem Motoru).

### 5. KullanÄ±m AlanlarÄ±
ETL Ä°ÅŸlemleri, Batch (Toplu) Raporlama.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** SQL KolaylÄ±ÄŸÄ±, Ã–lÃ§eklenebilirlik.
* **Dezavantajlar:** YÃ¼ksek Gecikme (GerÃ§ek zamanlÄ± deÄŸildir), Transaction kÄ±sÄ±tlÄ±dÄ±r."""
            },
            "link": "https://hive.apache.org/", "modern": False, "dep": "HDFS, Hadoop MR",
            "code": """-- HiveQL
CREATE TABLE sales (
    id INT, 
    amount DOUBLE
) STORED AS ORC;

SELECT SUM(amount) FROM sales;"""
        },
        "BigQuery": {
            "desc": {"en": "Serverless Data Warehouse.", "tr": "Sunucusuz Veri AmbarÄ±."},
            "detail": {
                "en": """### 1. Definition
BigQuery is a serverless, highly scalable, and cost-effective enterprise data warehouse offered on Google Cloud Platform (GCP).

### 2. Core Purpose
To query Terabytes of data in seconds using SQL without worrying about infrastructure management or capacity planning.

### 3. Architecture and Working Principle
Separates storage and compute.
* **Colossus:** Google's distributed file system.
* **Borg:** Cluster management system.
* **Columnar Storage:** Stores data in columns for compression and speed.

### 4. Key Components
* **Slots:** BigQuery's processing unit (Virtual CPU).
* **Project/Dataset/Table:** Data hierarchy.

### 5. Use Cases
Log Analytics, Real-time Analytics.

### 6. Pros and Cons
* **Pros:** Speed (Scans billions of rows in seconds), Serverless (No ops).
* **Cons:** Cost Uncertainty (Pay per query), Lock-in (GCP only).""",

                "tr": """### 1. TanÄ±m
BigQuery; Google Cloud Platform (GCP) Ã¼zerinde sunulan, sunucusuz (serverless), yÃ¼ksek Ã¶lÃ§eklenebilir ve uygun maliyetli bir kurumsal veri ambarÄ±dÄ±r.

### 2. Temel AmaÃ§
AltyapÄ± yÃ¶netimi ile uÄŸraÅŸmadan; saniyeler iÃ§inde Terabyte'larca veriyi SQL ile sorgulayabilmektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Depolama ve hesaplamayÄ± birbirinden ayÄ±rÄ±r.
* **Colossus:** Google'Ä±n daÄŸÄ±tÄ±k dosya sistemi.
* **Borg:** KÃ¼me yÃ¶netim sistemi.
* **Columnar Storage:** Veriyi sÃ¼tun bazlÄ± saklar.

### 4. Temel BileÅŸenler
Slots (Ä°ÅŸlem birimi), Dataset/Table hiyerarÅŸisi.

### 5. KullanÄ±m AlanlarÄ±
Log AnalitiÄŸi, GerÃ§ek ZamanlÄ± Analitik.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** HÄ±z, Serverless (SÄ±fÄ±r operasyonel yÃ¼k).
* **Dezavantajlar:** Maliyet BelirsizliÄŸi (Sorgu baÅŸÄ±na Ã¼cret), Lock-in."""
            },
            "link": "https://cloud.google.com/bigquery", "modern": True, "dep": "GCP",
            "code": """SELECT 
    start_station_name, 
    COUNT(*) as trips
FROM `bigquery-public-data.london_bicycles.cycle_hire`
GROUP BY 1
ORDER BY 2 DESC
LIMIT 10;"""
        },
        "CockroachDB": {
            "desc": {"en": "Distributed SQL.", "tr": "DaÄŸÄ±tÄ±k SQL (NewSQL)."},
            "detail": {
                "en": """### 1. Definition
CockroachDB is a cloud-native, distributed SQL database designed to be resilient against disasters.

### 2. Core Purpose
Named after the resilience of cockroaches; to survive disk, machine, rack, and even datacenter failures without data loss and keep running.

### 3. Architecture and Working Principle
Combines SQL features (JOIN, ACID) with NoSQL scalability.
* **Raft Consensus:** Ensures data consistency.
* **Ranges:** Splits data into 64MB chunks and distributes them automatically.

### 4. Key Components
Gateway Node, KV Store (RocksDB).

### 5. Use Cases
Global Applications (Multi-region), Financial Ledgers.

### 6. Pros and Cons
* **Pros:** Survivability (Resilient to failure), Geo-Partitioning.
* **Cons:** Performance (Higher latency than single-node Postgres), Resource intensive.""",

                "tr": """### 1. TanÄ±m
CockroachDB; bulut tabanlÄ±, daÄŸÄ±tÄ±k ve Ã¶zellikle felaketlere karÅŸÄ± dayanÄ±klÄ± (resilient) olacak ÅŸekilde tasarlanmÄ±ÅŸ bir DaÄŸÄ±tÄ±k SQL veritabanÄ±dÄ±r.

### 2. Temel AmaÃ§
Ä°smini hamambÃ¶ceklerinin dayanÄ±klÄ±lÄ±ÄŸÄ±ndan alÄ±r. Veri merkezi Ã§Ã¶kse bile veri kaybÄ± yaÅŸatmamak ve sistemi durdurmadan Ã§alÄ±ÅŸmaya devam etmektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
SQL Ã¶zelliklerini (ACID, JOIN) NoSQL Ã¶lÃ§eklenebilirliÄŸi ile birleÅŸtirir.
* **Raft:** Veri tutarlÄ±lÄ±ÄŸÄ±nÄ± saÄŸlar.
* **Ranges:** Veriyi 64MB'lÄ±k parÃ§alara bÃ¶ler ve daÄŸÄ±tÄ±r.

### 4. Temel BileÅŸenler
Gateway Node, KV Store (RocksDB).

### 5. KullanÄ±m AlanlarÄ±
Global Uygulamalar, Finansal KayÄ±t Sistemleri.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Hayatta Kalma (Felaket kurtarma), Geo-Partitioning (Veriyi Ã¼lkesinde tutma).
* **Dezavantajlar:** Performans (Gecikme daha yÃ¼ksektir), Kaynak kullanÄ±mÄ±."""
            },
            "link": "https://www.cockroachlabs.com/", "modern": True, "dep": None,
            "code": """-- Multi-region tablo
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    city STRING,
    name STRING
) LOCALITY REGIONAL BY ROW AS city;"""
        },
        "Druid": {
            "desc": {"en": "Real-time Analytics.", "tr": "GerÃ§ek ZamanlÄ± Analitik."},
            "detail": {
                "en": """### 1. Definition
Apache Druid is a high-performance data store designed for real-time analytics on large datasets with sub-second response times.

### 2. Core Purpose
To fill the gap of "interactive analytics" where data warehouses are too slow and operational databases lack analytical power. Optimized for Event data.

### 3. Architecture and Working Principle
Uses a Lambda-like architecture.
* **Real-time Nodes:** Ingest stream data immediately.
* **Historical Nodes:** Store deep storage data.
* **Broker:** Routes queries.

### 4. Key Components
Segment, Coordinator, Overlord.

### 5. Use Cases
Clickstream Analysis, APM (Application Performance Monitoring), Digital Advertising.

### 6. Pros and Cons
* **Pros:** Ultra Speed (Sub-second), Live Data querying.
* **Cons:** No Transactions (Updates are hard), Operational Overhead (Many components).""",

                "tr": """### 1. TanÄ±m
Apache Druid; bÃ¼yÃ¼k veri setleri Ã¼zerinde milisaniyenin altÄ±nda yanÄ±t sÃ¼releri ile gerÃ§ek zamanlÄ± analitik yapabilmek iÃ§in tasarlanmÄ±ÅŸ yÃ¼ksek performanslÄ± bir veri deposudur.

### 2. Temel AmaÃ§
Veri ambarlarÄ±nÄ±n yavaÅŸ kaldÄ±ÄŸÄ± "etkileÅŸimli analiz" boÅŸluÄŸunu doldurmaktÄ±r. Ã–zellikle "Olay" (Event) verisi iÃ§in optimize edilmiÅŸtir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Lambda mimarisine benzer.
* **Real-time Nodes:** Akan veriyi anÄ±nda alÄ±r ve sorgulanabilir kÄ±lar.
* **Historical Nodes:** Eski verileri saklar.
* **Broker:** Sorguyu daÄŸÄ±tÄ±r.

### 4. Temel BileÅŸenler
Segment, Coordinator, Overlord.

### 5. KullanÄ±m AlanlarÄ±
Clickstream (TÄ±klama) Analizi, Uygulama Performans Ä°zleme (APM), Dijital ReklamcÄ±lÄ±k.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Ultra HÄ±z, CanlÄ± Veri Sorgulama.
* **Dezavantajlar:** Transaction Yok, Operasyonel YÃ¼k (Ã‡ok fazla bileÅŸen)."""
            },
            "link": "https://druid.apache.org/", "modern": True, "dep": "Zookeeper",
            "code": """// JSON tabanlÄ± sorgu (Native)
{
  "queryType": "timeseries",
  "dataSource": "sample_data",
  "intervals": [ "2023-01-01/2023-01-02" ],
  "granularity": "hour"
}"""
        },
        "ClickHouse": {
            "desc": {"en": "Real-time OLAP.", "tr": "HÄ±zlÄ± Analitik DB."},
            "detail": {
                "en": "A column-oriented database management system aimed at interactive analytics. It allows querying billions of rows in milliseconds.",
                "tr": "Analitik sorgular iÃ§in optimize edilmiÅŸ sÃ¼tun tabanlÄ± veritabanÄ±. Milyarlarca satÄ±r Ã¼zerinde saniyeler iÃ§inde toplama (Sum, Avg) iÅŸlemi yapar. Log analitiÄŸi iÃ§in harikadÄ±r."
            },
            "link": "https://clickhouse.com/", "modern": True, "dep": "Zookeeper",
            "code": "SELECT Region, SUM(Sales) FROM orders GROUP BY Region;"
        },
        "Cassandra": {
            "desc": {"en": "Wide-Column.", "tr": "GeniÅŸ SÃ¼tunlu NoSQL."},
            "detail": {
                "en": "Distributed NoSQL database designed for handling large amounts of data across many commodity servers. Masterless architecture.",
                "tr": "Merkezi olmayan (Masterless) mimari. Sunuculardan biri Ã§Ã¶kse bile sistem Ã§alÄ±ÅŸÄ±r. Yazma hÄ±zÄ± Ã§ok yÃ¼ksektir. Facebook tarafÄ±ndan geliÅŸtirilmiÅŸtir."
            },
            "link": "https://cassandra.apache.org/", "modern": True, "dep": None,
            "code": "SELECT * FROM users WHERE user_id = 123;"
        },
        "HBase": {
            "desc": {"en": "Hadoop DB.", "tr": "Hadoop VeritabanÄ±."},
            "detail": {
                "en": "Modeled after Google BigTable. Runs on top of HDFS. Provides random read/write access to Big Data.",
                "tr": "HDFS Ã¼zerinde Ã§alÄ±ÅŸan, rastgele eriÅŸim saÄŸlayan NoSQL veritabanÄ±. Petabaytlarca veriyi saklayabilir."
            },
            "link": "https://hbase.apache.org/", "modern": False, "dep": "HDFS, Zookeeper",
            "code": "put 'table', 'row1', 'cf:col', 'value'"
        },
        "MongoDB": {
            "desc": {"en": "Document DB.", "tr": "DokÃ¼man NoSQL."},
            "detail": {
                "en": "Stores data in JSON-like documents. Flexible schema allows for rapid iteration and handling unstructured data.",
                "tr": "Veriyi JSON benzeri (BSON) dokÃ¼manlar olarak saklar. Esnek ÅŸemasÄ± sayesinde uygulama geliÅŸtirmeyi hÄ±zlandÄ±rÄ±r."
            },
            "link": "https://www.mongodb.com/", "modern": True, "dep": None,
            "code": "db.collection.find({ 'status': 'A' })"
        },
        "Redis": {
            "desc": {"en": "In-Memory.", "tr": "Bellek Ä°Ã§i."},
            "detail": {
                "en": "In-memory key-value store used as a database, cache, and message broker. Offers sub-millisecond response times.",
                "tr": "Veriyi RAM'de tutan anahtar-deÄŸer deposu. Ã–nbellek (Cache) olarak kullanÄ±lÄ±r. Mikrosaniye seviyesinde yanÄ±t verir."
            },
            "link": "https://redis.io/", "modern": True, "dep": None,
            "code": "SET user:100 'Omer'\nGET user:100"
        },
        "Elasticsearch": {
            "desc": {"en": "Search Engine.", "tr": "Arama Motoru."},
            "detail": {
                "en": "Distributed search engine built on Lucene. Stores data as JSON and uses inverted indices for fast full-text search.",
                "tr": "Lucene tabanlÄ± metin arama ve analiz motorudur. Ters dizin mantÄ±ÄŸÄ±yla Ã§ok hÄ±zlÄ± arama yapar. Log analitiÄŸi iÃ§in kullanÄ±lÄ±r."
            },
            "link": "https://www.elastic.co/", "modern": True, "dep": None,
            "code": "GET /_search?q=message:error"
        }
    },
    "Orchestration": {
        "Airflow": {
            "desc": {"en": "Workflow Orchestrator.", "tr": "Ä°ÅŸ AkÄ±ÅŸÄ± YÃ¶neticisi."},
            "detail": {
                "en": """### 1. Definition
Apache Airflow is an open-source workflow orchestration platform to programmatically author, schedule, and monitor workflows. Developed by Airbnb.

### 2. Core Purpose
To manage dependencies like "Do task A, if successful do B, else do C". Adopts the "Configuration as Code" principle.

### 3. Architecture and Working Principle
Airflow is based on the concept of **DAG** (Directed Acyclic Graph).
* **Scheduler:** Triggers scheduled tasks.
* **Executor:** Determines where tasks run (Local, Kubernetes, Celery).
* **Web Server:** Provides a UI to monitor tasks.

### 4. Key Components
* **DAG:** Workflow definition in Python code.
* **Operator:** Template determining what a task does (PythonOperator, BashOperator).
* **Task:** A running instance of an Operator.
* **Metadata Database:** Stores the state of all tasks.

### 5. Use Cases
ETL Automation, ML Pipelines.

### 6. Pros and Cons
* **Pros:** Python based, Huge community, Extensible.
* **Cons:** Not real-time (Batch focused), Data Awareness limited (Task-based, not data-based).""",

                "tr": """### 1. TanÄ±m
Apache Airflow; karmaÅŸÄ±k veri iÅŸ akÄ±ÅŸlarÄ±nÄ± (workflows) programatik olarak oluÅŸturmak, zamanlamak ve izlemek iÃ§in geliÅŸtirilmiÅŸ, Python tabanlÄ±, aÃ§Ä±k kaynaklÄ± bir iÅŸ akÄ±ÅŸÄ± orkestrasyon platformudur. Airbnb tarafÄ±ndan geliÅŸtirilmiÅŸtir.

### 2. Temel AmaÃ§
Veri mÃ¼hendisliÄŸinde "Ã–nce A iÅŸini yap, biterse B'yi yap, hata alÄ±rsa C'yi yap" ÅŸeklindeki baÄŸÄ±mlÄ±lÄ±klarÄ± (dependency) yÃ¶netmektir. "Configuration as Code" (Kod olarak konfigÃ¼rasyon) prensibini benimser.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Airflow'un kalbinde **DAG** (YÃ¶nlÃ¼ DÃ¶ngÃ¼sÃ¼z Ã‡izge) kavramÄ± yatar.
* **Scheduler:** Planlanan zamanÄ± gelen iÅŸleri tetikler.
* **Executor:** Ä°ÅŸin nerede Ã§alÄ±ÅŸacaÄŸÄ±nÄ± belirler (Local, Kubernetes, Celery vb.).
* **Web Server:** Ä°ÅŸlerin durumunu izlemek iÃ§in gÃ¶rsel arayÃ¼z sunar.

### 4. Temel BileÅŸenler
* **DAG:** Ä°ÅŸ akÄ±ÅŸÄ±nÄ±n Python kodu ile tanÄ±mÄ±.
* **Operator:** Bir iÅŸin ne yapacaÄŸÄ±nÄ± belirleyen ÅŸablon (PythonOperator, BashOperator).
* **Task:** Operator'Ã¼n Ã§alÄ±ÅŸan hali.
* **Metadata Database:** TÃ¼m iÅŸlerin durumunu tutan veritabanÄ±.

### 5. KullanÄ±m AlanlarÄ±
ETL Otomasyonu, ML Pipeline (Model eÄŸitimi).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Python tabanlÄ±, GeniÅŸ topluluk.
* **Dezavantajlar:** Gecikme (Batch odaklÄ±dÄ±r), Veri FarkÄ±ndalÄ±ÄŸÄ± dÃ¼ÅŸÃ¼ktÃ¼r (Ä°ÅŸ bitti der, veri doÄŸru mu bilmez)."""
            },
            "link": "https://airflow.apache.org/", "modern": True, "dep": "DB",
            "code": """from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG('my_pipeline', start_date=datetime(2023, 1, 1)) as dag:
    
    t1 = BashOperator(
        task_id='extract',
        bash_command='python extract.py'
    )

    t2 = BashOperator(
        task_id='load',
        bash_command='python load.py'
    )

    t1 >> t2  # t1 bittikten sonra t2 baÅŸlasÄ±n"""
        },
        "Kubernetes": {
            "desc": {"en": "Container Orchestration.", "tr": "Konteyner Orkestrasyonu."},
            "detail": {
                "en": """### 1. Definition
Kubernetes (K8s) is an open-source container orchestration system for automating application deployment, scaling, and management. Developed by Google.

### 2. Core Purpose
To manage hundreds or thousands of containers like a "conductor". If a server fails, it automatically restarts containers on another server (Self-healing).

### 3. Architecture and Working Principle
Works on the "Desired State" principle.
* **Control Plane (Master):** The brain making decisions.
* **Worker Nodes:** Servers where the work happens.

### 4. Key Components
* **Pod:** The smallest unit in K8s. Holds one or more containers.
* **Kubelet:** Agent running on each node ensuring Pods are healthy.
* **etcd:** Key-value store holding cluster data.
* **Service:** Network layer exposing Pods.

### 5. Use Cases
Microservices, Big Data on K8s (Spark, Flink).

### 6. Pros and Cons
* **Pros:** Portability, Auto-scaling, Self-healing.
* **Cons:** Steep learning curve, Complexity overhead.""",

                "tr": """### 1. TanÄ±m
Kubernetes; konteynerize edilmiÅŸ uygulamalarÄ±n daÄŸÄ±tÄ±mÄ±nÄ±, Ã¶lÃ§eklendirilmesini ve yÃ¶netimini otomatize eden, aÃ§Ä±k kaynaklÄ± bir konteyner orkestrasyon sistemidir. Google tarafÄ±ndan geliÅŸtirilmiÅŸtir.

### 2. Temel AmaÃ§
YÃ¼zlerce konteyneri bir "orkestra ÅŸefi" gibi yÃ¶netmektir. Bir sunucu Ã§Ã¶kerse, Ã¼zerindeki konteynerleri otomatik olarak baÅŸka sunucuda baÅŸlatÄ±r (Self-healing).

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
"Arzulanan Durum" (Desired State) prensibiyle Ã§alÄ±ÅŸÄ±r.
* **Control Plane (Master):** Beyin takÄ±mÄ±dÄ±r. KararlarÄ± verir.
* **Worker Nodes:** Ä°ÅŸin yapÄ±ldÄ±ÄŸÄ± sunuculardÄ±r.

### 4. Temel BileÅŸenler
* **Pod:** Kubernetes'in en kÃ¼Ã§Ã¼k birimidir. Ä°Ã§inde bir veya daha fazla konteyner barÄ±ndÄ±rÄ±r.
* **Kubelet:** Her sunucuda Ã§alÄ±ÅŸan ajan.
* **etcd:** KÃ¼menin tÃ¼m bilgilerini tutan anahtar-deÄŸer deposu.
* **Service:** AÄŸ katmanÄ±.

### 5. KullanÄ±m AlanlarÄ±
Mikroservisler, K8s Ã¼zerinde Big Data (Spark, Flink).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** TaÅŸÄ±nabilirlik, Otomatik Ã–lÃ§ekleme.
* **Dezavantajlar:** KarmaÅŸÄ±klÄ±k, Ã–ÄŸrenme eÄŸrisi Ã§ok diktir."""
            },
            "link": "https://kubernetes.io/", "modern": True, "dep": "Docker",
            "code": """# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80"""
        },
        "Docker": {
            "desc": {"en": "Containerization.", "tr": "KonteynerleÅŸtirme."},
            "detail": {
                "en": """### 1. Definition
Docker is a platform that packages applications with all their dependencies into standardized units called containers.

### 2. Core Purpose
To solve the "it works on my machine" problem (Dependency Hell). Provides lightweight isolation compared to VMs.

### 3. Architecture and Working Principle
Shares the OS Kernel but isolates processes.
* **Image:** Read-only template of the application.
* **Container:** Running instance of an image.

### 4. Key Components
* **Dockerfile:** Recipe for building an image.
* **Docker Daemon:** Background process managing containers.
* **Docker Hub:** Registry for images.

### 5. Use Cases
CI/CD, Microservices.

### 6. Pros and Cons
* **Pros:** Speed (Starts in seconds), Efficiency.
* **Cons:** Security (Shared kernel), Ephemeral data.""",

                "tr": """### 1. TanÄ±m
Docker; uygulamalarÄ± tÃ¼m baÄŸÄ±mlÄ±lÄ±klarÄ± ile birlikte paketleyerek her ortamda aynÄ± ÅŸekilde Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayan konteynerizasyon platformudur.

### 2. Temel AmaÃ§
"Benim makinemde Ã§alÄ±ÅŸÄ±yordu" sorununu ortadan kaldÄ±rmaktÄ±r. Sanal makinelere (VM) gÃ¶re Ã§ok daha hafif bir izolasyon saÄŸlar.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Ä°ÅŸletim sistemi Ã§ekirdeÄŸini (Kernel) paylaÅŸÄ±r, sÃ¼reÃ§leri izole eder.
* **Image (Ä°maj):** UygulamanÄ±n dondurulmuÅŸ ÅŸablonudur.
* **Container:** Ä°majÄ±n Ã§alÄ±ÅŸan halidir.

### 4. Temel BileÅŸenler
* **Dockerfile:** Ä°maj reÃ§etesi.
* **Docker Daemon:** Konteynerleri yÃ¶neten motor.
* **Docker Hub:** Ä°maj deposu.

### 5. KullanÄ±m AlanlarÄ±
CI/CD, Mikroservisler.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** HÄ±z (Saniyeler iÃ§inde baÅŸlar), Verimlilik.
* **Dezavantajlar:** GÃ¼venlik (Kernel paylaÅŸÄ±mÄ±), KalÄ±cÄ±lÄ±k (Konteyner silinince veri gider)."""
            },
            "link": "https://www.docker.com/", "modern": True, "dep": None,
            "code": """# Dockerfile Ã–rneÄŸi
FROM python:3.9-slim

WORKDIR /app
COPY . .
RUN pip install -r requirements.txt

CMD ["python", "app.py"]"""
        },
        "Dagster": {
            "desc": {"en": "Data Orchestrator.", "tr": "Veri OdaklÄ± OrkestratÃ¶r."},
            "detail": {
                "en": """### 1. Definition
Dagster is a next-generation data orchestrator designed around data assets (tables, ML models) rather than tasks.

### 2. Core Purpose
To solve the "Data Awareness" problem of Airflow. Instead of "Task A -> Task B", it focuses on "Table A updated -> Produce Table B".

### 3. Architecture and Working Principle
Uses "Asset-based Orchestration". Focuses on the produced data (Asset).

### 4. Key Components
* **Asset:** The data produced.
* **Op:** The function processing data.
* **Dagit:** Advanced UI.

### 5. Use Cases
Modern Data Stack, Data Quality testing.

### 6. Pros and Cons
* **Pros:** Data-centric, Testability.
* **Cons:** Newer ecosystem than Airflow.""",

                "tr": """### 1. TanÄ±m
Dagster; veri varlÄ±klarÄ±nÄ± (Data Assets - Tablolar, ML Modelleri) merkeze alan, yeni nesil bir veri orkestratÃ¶rÃ¼dÃ¼r.

### 2. Temel AmaÃ§
Airflow'un "Veri FarkÄ±ndalÄ±ÄŸÄ±" eksiÄŸini Ã§Ã¶zmektir. GÃ¶rev sÄ±rasÄ±na deÄŸil, Ã¼retilen veriye (Asset) odaklanÄ±r. YazÄ±lÄ±m mÃ¼hendisliÄŸi prensiplerini veri dÃ¼nyasÄ±na getirir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
"VarlÄ±k tabanlÄ± orkestrasyon" (Asset-based Orchestration) kullanÄ±r.

### 4. Temel BileÅŸenler
* **Asset:** Ãœretilen veri (Ã–rn: users_table).
* **Op:** Veriyi iÅŸleyen fonksiyon.
* **Dagit:** GÃ¶rsel arayÃ¼z.

### 5. KullanÄ±m AlanlarÄ±
Modern Data Stack, Veri Kalitesi testleri.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Veri OdaklÄ±, Test Edilebilirlik.
* **Dezavantajlar:** PopÃ¼larite (HenÃ¼z Airflow kadar yaygÄ±n deÄŸil)."""
            },
            "link": "https://dagster.io/", "modern": True, "dep": None,
            "code": """from dagster import asset

@asset
def my_table():
    return [1, 2, 3]

@asset
def derived_table(my_table):
    return [x * 2 for x in my_table]"""
        },
        "Zookeeper": {
            "desc": {"en": "Distributed Coordination.", "tr": "Merkezi Koordinasyon Servisi."},
            "detail": {
                "en": """### 1. Definition
Apache ZooKeeper is a high-performance coordination service for distributed applications.

### 2. Core Purpose
Solves hard problems like Synchronization, Leader Election, and Configuration Management in distributed systems.

### 3. Architecture and Working Principle
Uses a file-system-like namespace. Keeps data in RAM for speed.
* **Leader/Follower:** One leader handles writes, others follow.

### 4. Key Components
* **Znode:** Data node.
* **Ensemble:** Cluster of ZooKeeper servers.

### 5. Use Cases
Kafka metadata management, Hadoop HA.

### 6. Pros and Cons
* **Pros:** Reliability, Simplicity.
* **Cons:** Management complexity (Java based), Modern tools are replacing it.""",

                "tr": """### 1. TanÄ±m
Apache ZooKeeper; daÄŸÄ±tÄ±k uygulamalar iÃ§in yÃ¼ksek performanslÄ± bir koordinasyon servisidir.

### 2. Temel AmaÃ§
DaÄŸÄ±tÄ±k sistemlerdeki Senkronizasyon, Lider SeÃ§imi ve KonfigÃ¼rasyon YÃ¶netimi gibi zor problemleri Ã§Ã¶zer. Kafka ve Hadoop'un "trafik polisi"dir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Dosya sistemine benzer bir yapÄ± kullanÄ±r. Veriyi RAM'de tutar.
* **Leader/Follower:** Yazma iÅŸlemlerini Lider yapar, diÄŸerleri takip eder.

### 4. Temel BileÅŸenler
* **Znode:** Veri dÃ¼ÄŸÃ¼mÃ¼.
* **Ensemble:** ZooKeeper kÃ¼mesi.

### 5. KullanÄ±m AlanlarÄ±
Kafka (Eski sÃ¼rÃ¼mler), Hadoop HA.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** GÃ¼venilirlik.
* **Dezavantajlar:** YÃ¶netim ZorluÄŸu, Eskime (Yerini KRaft ve etcd alÄ±yor)."""
            },
            "link": "https://zookeeper.apache.org/", "modern": False, "dep": "JVM",
            "code": """# Zookeeper CLI
ls /brokers/ids
get /config/topics/my-topic"""
        },
        "YARN": {
            "desc": {"en": "Resource Manager.", "tr": "Kaynak YÃ¶neticisi."},
            "detail": {
                "en": """### 1. Definition
YARN (Yet Another Resource Negotiator) is Hadoop's resource management layer.

### 2. Core Purpose
To manage RAM and CPU resources in a cluster and share them among different applications like Spark, MapReduce, Hive.

### 3. Architecture
* **ResourceManager:** Master. Manages global resources.
* **NodeManager:** Slave. Runs on each node.

### 4. Key Components
ResourceManager, NodeManager, ApplicationMaster.

### 5. Use Cases
Hadoop Clusters (Multi-tenant).

### 6. Pros and Cons
* **Pros:** Efficiency, Multi-tenancy.
* **Cons:** Declining popularity due to K8s.""",

                "tr": """### 1. TanÄ±m
YARN; Hadoop ekosisteminin kaynak yÃ¶netim katmanÄ±dÄ±r. "Big Data'nÄ±n Ä°ÅŸletim Sistemi" olarak anÄ±lÄ±r.

### 2. Temel AmaÃ§
Bir sunucu kÃ¼mesindeki RAM ve CPU kaynaklarÄ±nÄ± yÃ¶netmek ve bunlarÄ± farklÄ± uygulamalar (Spark, Hive) arasÄ±nda paylaÅŸtÄ±rmaktÄ±r.

### 3. Mimari
* **ResourceManager:** Patron (Master).
* **NodeManager:** Ä°ÅŸÃ§i (Slave).

### 4. Temel BileÅŸenler
ResourceManager, NodeManager, ApplicationMaster.

### 5. KullanÄ±m AlanlarÄ±
Hadoop KÃ¼meleri.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Verimlilik, Ã‡oklu KiracÄ±.
* **Dezavantajlar:** Modernite (Kubernetes karÅŸÄ±sÄ±nda popÃ¼laritesi azalÄ±yor)."""
            },
            "link": "https://hadoop.apache.org/", "modern": False, "dep": None,
            "code": """# YARN CLI
yarn application -list
yarn node -list"""
        },
        "Prometheus": {
            "desc": {"en": "Monitoring System.", "tr": "Sistem Ä°zleme AracÄ±."},
            "detail": {
                "en": """### 1. Definition
Prometheus is an open-source systems monitoring and alerting toolkit. Standard for K8s monitoring.

### 2. Core Purpose
To collect and store time-series data (metrics) and alert on issues.

### 3. Architecture and Working Principle
Uses a **Pull Model**. It scrapes metrics from targets. Stores data in its own TSDB.

### 4. Key Components
* **Server:** Collects and stores data.
* **Exporters:** Agents that expose metrics.
* **Alertmanager:** Handles alerts.

### 5. Use Cases
Kubernetes Monitoring, Infrastructure Monitoring.

### 6. Pros and Cons
* **Pros:** Speed, K8s native.
* **Cons:** Long-term storage (Needs Thanos), Visualization (Needs Grafana).""",

                "tr": """### 1. TanÄ±m
Prometheus; Ã¶zellikle mikroservis ve konteyner yapÄ±larÄ± iÃ§in tasarlanmÄ±ÅŸ, aÃ§Ä±k kaynaklÄ± sistem izleme ve uyarÄ± aracÄ±dÄ±r.

### 2. Temel AmaÃ§
Zaman serisi verilerini (CPU, RAM, Request Count) toplamak ve sorun olduÄŸunda uyarÄ± vermektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
**Pull Model (Ã‡ekme)** kullanÄ±r. Hedeflere gidip veriyi kendisi alÄ±r. Veriyi kendi Ã¶zel veritabanÄ±nda saklar.

### 4. Temel BileÅŸenler
Prometheus Server, Exporters (Veri ajanlarÄ±), Alertmanager.

### 5. KullanÄ±m AlanlarÄ±
Kubernetes Ä°zleme, AltyapÄ± Ä°zleme.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** HÄ±z, K8s Uyumu.
* **Dezavantajlar:** Uzun SÃ¼reli Saklama (Thanos gerekir), GÃ¶rsellik (Grafana gerekir)."""
            },
            "link": "https://prometheus.io/", "modern": True, "dep": None,
            "code": """# prometheus.yml
scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']"""
        },
        "cAdvisor": {
            "desc": {"en": "Container Monitoring.", "tr": "Konteyner Kaynak Ä°zleme."},
            "detail": {
                "en": """### 1. Definition
cAdvisor (Container Advisor) provides container users an understanding of the resource usage and performance characteristics of their running containers.

### 2. Core Purpose
To collect real-time metrics (RAM, CPU) from individual containers.

### 3. Architecture
Runs as a daemon on each node. Scrapes data from Docker.

### 4. Components
Web UI, REST API.

### 5. Use Cases
Source for Prometheus scraping.

### 6. Pros and Cons
* **Pros:** Simple, Auto-discovery.
* **Cons:** No Storage (Needs DB).""",

                "tr": """### 1. TanÄ±m
cAdvisor; Ã§alÄ±ÅŸan konteynerlerin kaynak kullanÄ±mÄ±nÄ± ve performans Ã¶zelliklerini analiz eden hafif bir ajandÄ±r.

### 2. Temel AmaÃ§
Tekil bir sunucu Ã¼zerindeki konteynerlerin anlÄ±k CPU/RAM bilgilerini toplamaktÄ±r.

### 3. Mimari
Her sunucuda bir tane Ã§alÄ±ÅŸÄ±r. Docker ile konuÅŸur.

### 4. Temel BileÅŸenler
Web UI, REST API.

### 5. KullanÄ±m AlanlarÄ±
Prometheus iÃ§in veri kaynaÄŸÄ±.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Basitlik, Otomatik KeÅŸif.
* **Dezavantajlar:** Depolama Yok (Veriyi saklamaz, sadece anlÄ±k gÃ¶sterir)."""
            },
            "link": "https://github.com/google/cadvisor", "modern": True, "dep": "Docker",
            "code": """# Docker ile Ã‡alÄ±ÅŸtÄ±rma
docker run \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  google/cadvisor:latest"""
        }
    },
    "Serving/BI": {
        "Streamlit": {
            "desc": {"en": "Data Apps Framework.", "tr": "Veri Uygulama Ã‡atÄ±sÄ±."},
            "detail": {
                "en": """### 1. Definition
Streamlit is an open-source app framework designed specifically for Data Scientists and Machine Learning Engineers to create interactive web apps using only Python.

### 2. Core Purpose
To allow data scientists to turn their analysis, charts, or ML models into shareable web apps ("Data Apps") in seconds without needing to know HTML, CSS, or JavaScript (Frontend).

### 3. Architecture and Working Principle
Streamlit relies on the "Script-run" principle.
* When a user interacts with a widget, Streamlit reruns the entire Python script from top to bottom.
* It uses React.js in the background but abstracts it away from the developer.

### 4. Key Components
* **Widgets:** Buttons, sliders, text inputs.
* **Caching:** Mechanism (@st.cache) to prevent re-running heavy computations.
* **Session State:** Persists variables across reruns.

### 5. Use Cases
ML Model Prototyping, Internal Tools.

### 6. Pros and Cons
* **Pros:** Speed (Build an app in 50 lines), Pure Python.
* **Cons:** Customization limits (Layout is rigid), Not for massive enterprise apps.""",

                "tr": """### 1. TanÄ±m
Streamlit; Ã¶zellikle Veri Bilimciler ve Makine Ã–ÄŸrenimi mÃ¼hendisleri iÃ§in tasarlanmÄ±ÅŸ, sadece Python kodu yazarak interaktif web uygulamalarÄ± oluÅŸturmayÄ± saÄŸlayan aÃ§Ä±k kaynaklÄ± bir uygulama Ã§atÄ±sÄ±dÄ±r.

### 2. Temel AmaÃ§
Bir veri bilimcinin, Frontend (HTML/CSS/JS) bilmesine gerek kalmadan; analizlerini veya ML modellerini saniyeler iÃ§inde paylaÅŸÄ±labilir bir web uygulamasÄ±na ("Data App") dÃ¶nÃ¼ÅŸtÃ¼rmesini saÄŸlamaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Streamlit'in Ã§alÄ±ÅŸma mantÄ±ÄŸÄ± "Script-run" prensibine dayanÄ±r.
* KullanÄ±cÄ± bir etkileÅŸimde bulunduÄŸunda, Streamlit tÃ¼m Python kodunu baÅŸtan sona tekrar Ã§alÄ±ÅŸtÄ±rÄ±r.
* Arka planda React.js kullanÄ±r ancak geliÅŸtirici sadece saf Python yazar.

### 4. Temel BileÅŸenler
* **Widgets:** Butonlar, kaydÄ±rÄ±cÄ±lar, metin kutularÄ±.
* **Caching:** AÄŸÄ±r iÅŸlemleri tekrar yapmamak iÃ§in Ã¶nbellek mekanizmasÄ± (@st.cache).
* **Session State:** EtkileÅŸimler sÄ±rasÄ±nda deÄŸiÅŸkenlerin deÄŸerini koruyan yapÄ±.

### 5. KullanÄ±m AlanlarÄ±
ML Model Prototipleme, Åirket Ä°Ã§i AraÃ§lar.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** HÄ±z (50 satÄ±rla uygulama), Sadece Python bilgisi yeterli.
* **Dezavantajlar:** Ã–zelleÅŸtirme kÄ±sÄ±tlÄ±dÄ±r, Ã‡ok bÃ¼yÃ¼k uygulamalar iÃ§in uygun deÄŸildir."""
            },
            "link": "https://streamlit.io/", "modern": True, "dep": "Python",
            "code": """import streamlit as st
import pandas as pd

st.title("Veri Analiz Paneli")
st.write("Veri setinizi yÃ¼kleyin:")

uploaded_file = st.file_uploader("CSV SeÃ§", type="csv")

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write(df)
    st.bar_chart(df.select_dtypes(include=['number']))"""
        },
        "Superset": {
            "desc": {"en": "Modern Open Source BI.", "tr": "Modern AÃ§Ä±k Kaynak Ä°ÅŸ ZekasÄ±."},
            "detail": {
                "en": """### 1. Definition
Apache Superset is an enterprise-grade open-source Business Intelligence (BI) platform designed for modern data stacks to explore and visualize large-scale data.

### 2. Core Purpose
To provide a powerful open-source alternative to expensive proprietary BI tools (Tableau, PowerBI) and enable SQL-savvy users to deeply analyze data.

### 3. Architecture and Working Principle
It has a Cloud-native architecture. Uses Python (Flask) for the backend.
* **SQLAlchemy:** Uses this ORM to connect to almost any SQL-speaking database.
* **Caching:** Caches query results to improve performance.

### 4. Key Components
* **SQL Lab:** Advanced SQL editor.
* **Semantic Layer:** Layer to define virtual metrics and calculated columns.
* **Explore:** No-code chart creation interface.

### 5. Use Cases
Modern Data Teams (Airbnb, Netflix), Big Data Visualization (Geo-spatial).

### 6. Pros and Cons
* **Pros:** Huge connectivity (Hive, Presto, Druid), Rich visualizations (Deck.gl).
* **Cons:** Learning curve (Technical), Complex JOINs are harder in UI.""",

                "tr": """### 1. TanÄ±m
Apache Superset; modern veri yÄ±ÄŸÄ±nlarÄ± iÃ§in geliÅŸtirilmiÅŸ, bÃ¼yÃ¼k Ã¶lÃ§ekli verileri keÅŸfetmeye ve gÃ¶rselleÅŸtirmeye yarayan, kurumsal seviyede aÃ§Ä±k kaynaklÄ± bir Ä°ÅŸ ZekasÄ± (BI) platformudur.

### 2. Temel AmaÃ§
PahalÄ± lisanslÄ± BI araÃ§larÄ±na gÃ¼Ã§lÃ¼ bir aÃ§Ä±k kaynak alternatif sunmak; SQL bilen kullanÄ±cÄ±larÄ±n veritabanÄ±ndaki veriyi derinlemesine analiz etmesini saÄŸlamaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Bulut tabanlÄ± bir mimariye sahiptir. Web sunucusu olarak Python (Flask) kullanÄ±r.
* **SQLAlchemy:** VeritabanÄ± baÄŸlantÄ±larÄ± iÃ§in bu kÃ¼tÃ¼phaneyi kullanÄ±r, SQL konuÅŸan her ÅŸeye baÄŸlanÄ±r.
* **Caching:** Sorgu sonuÃ§larÄ±nÄ± Ã¶nbelleÄŸe alÄ±r.

### 4. Temel BileÅŸenler
* **SQL Lab:** GeliÅŸmiÅŸ SQL editÃ¶rÃ¼.
* **Semantic Layer:** Sanal metrik tanÄ±mlama katmanÄ±.
* **Explore:** Kod yazmadan grafik oluÅŸturma arayÃ¼zÃ¼.

### 5. KullanÄ±m AlanlarÄ±
Modern Veri Ekipleri, BÃ¼yÃ¼k Veri GÃ¶rselleÅŸtirme.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** GeniÅŸ BaÄŸlantÄ± (Hive, Presto, Snowflake), Zengin GÃ¶rsellik.
* **Dezavantajlar:** Ã–ÄŸrenme EÄŸrisi (Teknik bilgi ister), ArayÃ¼zde JOIN yapmak zordur."""
            },
            "link": "https://superset.apache.org/", "modern": True, "dep": None,
            "code": """-- SQL Lab Sorgusu
SELECT 
    country_name,
    COUNT(*) as user_count
FROM users
WHERE signup_date >= '2023-01-01'
GROUP BY country_name
ORDER BY user_count DESC
LIMIT 10;"""
        },
        "Tableau": {
            "desc": {"en": "Visual Analytics Platform.", "tr": "GÃ¶rsel Analitik Platformu."},
            "detail": {
                "en": """### 1. Definition
Tableau is a market-leading visual analytics platform that makes data analysis accessible via a drag-and-drop interface. Owned by Salesforce.

### 2. Core Purpose
To help people "see and understand data". Enables non-technical users to create their own reports (Self-Service BI) without IT support.

### 3. Architecture and Working Principle
Powered by **VizQL** (Visual Query Language).
* When a user drags a field, VizQL converts this action into an optimized database query and renders the result as a chart.
* Supports **Live** connection or **Extract** (Hyper Engine).

### 4. Key Components
Desktop, Server/Cloud, Prep (Data cleaning).

### 5. Use Cases
Executive Dashboards, Financial Reporting.

### 6. Pros and Cons
* **Pros:** Ease of use, Huge Community.
* **Cons:** High Cost, Performance on massive live data.""",

                "tr": """### 1. TanÄ±m
Tableau; veriyi analiz etmeyi ve gÃ¶rselleÅŸtirmeyi herkes iÃ§in eriÅŸilebilir kÄ±lan, sÃ¼rÃ¼kle-bÄ±rak mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸan, pazar lideri bir gÃ¶rsel analitik platformudur.

### 2. Temel AmaÃ§
Ä°nsanlarÄ±n veriyi "gÃ¶rmesini ve anlamasÄ±nÄ±" saÄŸlamaktÄ±r. Teknik olmayan kullanÄ±cÄ±larÄ±n kendi raporlarÄ±nÄ± (Self-Service BI) hazÄ±rlamasÄ±na olanak tanÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Kalbinde **VizQL** teknolojisi yatar.
* KullanÄ±cÄ± ekrana bir tabloyu sÃ¼rÃ¼klediÄŸinde, VizQL bunu arka planda optimize edilmiÅŸ bir veritabanÄ± sorgusuna dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve grafiÄŸi Ã§izer.
* **CanlÄ± (Live)** veya **Ã–zet (Extract)** modunda Ã§alÄ±ÅŸabilir.

### 4. Temel BileÅŸenler
Tableau Desktop, Server/Cloud, Prep (Veri temizleme).

### 5. KullanÄ±m AlanlarÄ±
YÃ¶netici DashboardlarÄ±, Finansal Raporlama.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** KullanÄ±m KolaylÄ±ÄŸÄ±, GeniÅŸ Topluluk.
* **Dezavantajlar:** YÃ¼ksek Maliyet, Ã‡ok bÃ¼yÃ¼k canlÄ± verilerde performans."""
            },
            "link": "https://www.tableau.com/", "modern": True, "dep": None,
            "code": """# Tableau Prep Flow (Kavramsal)
Input(Sales_Data) -> Clean(Remove Nulls) -> Aggregate(Sum Sales by Region) -> Output(Hyper File)"""
        },
        "Grafana": {
            "desc": {"en": "Observability Platform.", "tr": "GÃ¶zlemlenebilirlik Paneli."},
            "detail": {
                "en": """### 1. Definition
Grafana is an open-source Observability platform used to query, visualize, and alert on metrics, logs, and traces regardless of where they are stored.

### 2. Core Purpose
To monitor the health of systems, servers, apps, or IoT devices from a "Single Pane of Glass". Focuses on "Operational Data" (CPU, RAM) rather than Business Data.

### 3. Architecture and Working Principle
Grafana does not store data itself. It pulls data from sources in real-time.
* **Plugin Architecture:** Connects to dozens of sources (Prometheus, InfluxDB, MySQL) simultaneously.

### 4. Key Components
Dashboard, Alerting Engine, Loki/Tempo (Logs/Traces).

### 5. Use Cases
DevOps Monitoring (K8s), Industrial IoT.

### 6. Pros and Cons
* **Pros:** Flexibility, Visualization (Dark mode friendly).
* **Cons:** No Data Storage (Relies on source), Learning curve for queries.""",

                "tr": """### 1. TanÄ±m
Grafana; metrikleri, loglarÄ± ve izleri nerede tutulduklarÄ±na bakÄ±lmaksÄ±zÄ±n sorgulamak, gÃ¶rselleÅŸtirmek ve uyarÄ± Ã¼retmek iÃ§in kullanÄ±lan aÃ§Ä±k kaynaklÄ± bir GÃ¶zlemlenebilirlik platformudur.

### 2. Temel AmaÃ§
Sistemlerin, sunucularÄ±n veya IoT cihazlarÄ±nÄ±n saÄŸlÄ±ÄŸÄ±nÄ± tek bir ekrandan izlemektir. "Ä°ÅŸ verisi"nden ziyade "Operasyonel veri"ye (CPU, RAM) odaklanÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Grafana veriyi kendisi saklamaz. Veriyi kaynaÄŸÄ±ndan anlÄ±k Ã§eker.
* **Plugin Mimarisi:** Prometheus, InfluxDB, Elasticsearch gibi onlarca kaynaÄŸa aynÄ± anda baÄŸlanabilir.

### 4. Temel BileÅŸenler
Dashboard, Alerting Engine (UyarÄ± Motoru), Loki/Tempo.

### 5. KullanÄ±m AlanlarÄ±
DevOps Ä°zleme (K8s), EndÃ¼striyel IoT.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Esneklik (Her veritabanÄ±nÄ± birleÅŸtirir), GÃ¶rsellik.
* **Dezavantajlar:** Veri Saklamaz, KaynaÄŸa baÄŸÄ±mlÄ±dÄ±r."""
            },
            "link": "https://grafana.com/", "modern": True, "dep": None,
            "code": """// PromQL Sorgusu (Grafana Panelinde)
rate(http_requests_total{status="500"}[5m]) > 10"""
        },
        "Metabase": {
            "desc": {"en": "Simple BI.", "tr": "Basit Ä°ÅŸ ZekasÄ±."},
            "detail": {
                "en": """### 1. Definition
Metabase is a user-friendly, open-source BI tool that installs in minutes and lets non-technical users ask questions about data.

### 2. Core Purpose
To solve the "Data Bottleneck". Enables Marketing or HR teams to ask their own questions (Democratization of Data) instead of waiting for the data team.

### 3. Architecture and Working Principle
Java (Clojure) based. Runs as a single .jar file or Docker image. Scans database schema to understand relationships.

### 4. Key Components
* **Query Builder:** Visual interface to filter, group, and summarize without code.
* **Pulse:** Automated reporting via Email/Slack.

### 5. Use Cases
Startups, Simple operational reporting.

### 6. Pros and Cons
* **Pros:** Extremely simple setup and usage, Free open-source version.
* **Cons:** Limited depth for very complex queries.""",

                "tr": """### 1. TanÄ±m
Metabase; kurulumu dakikalar sÃ¼ren, teknik olmayan kullanÄ±cÄ±larÄ±n basit sorular sorarak veriye ulaÅŸmasÄ±nÄ± saÄŸlayan, kullanÄ±cÄ± dostu ve aÃ§Ä±k kaynaklÄ± bir BI aracÄ±dÄ±r.

### 2. Temel AmaÃ§
Åirketlerdeki "Veri darboÄŸazÄ±nÄ±" Ã§Ã¶zmektir. Ekiplerin kendi raporlarÄ±nÄ± kendilerinin hazÄ±rlamasÄ±nÄ± (Verinin DemokratikleÅŸmesi) hedefler.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Java (Clojure) tabanlÄ±dÄ±r. Tek bir dosya veya Docker imajÄ± olarak Ã§alÄ±ÅŸÄ±r. VeritabanÄ± ÅŸemasÄ±nÄ± tarar.

### 4. Temel BileÅŸenler
Query Builder (GÃ¶rsel Sorgu OluÅŸturucu), Pulse (Otomatik Raporlama).

### 5. KullanÄ±m AlanlarÄ±
Startup'lar, Basit Analizler.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Basitlik (En kolay BI aracÄ±), Ãœcretsiz versiyon.
* **Dezavantajlar:** Derinlik (KarmaÅŸÄ±k veri modellerinde yetersiz kalabilir)."""
            },
            "link": "https://www.metabase.com/", "modern": True, "dep": None,
            "code": """# Docker ile Metabase Ã‡alÄ±ÅŸtÄ±rma
docker run -d -p 3000:3000 --name metabase metabase/metabase"""
        },
        "Kibana": {
            "desc": {"en": "ES Visualization.", "tr": "Elasticsearch GÃ¶rselleÅŸtirme."},
            "detail": {
                "en": """### 1. Definition
Kibana is the visualization and management interface for the Elastic Stack (ELK), designed specifically for Elasticsearch data.

### 2. Core Purpose
To facilitate "searching for a needle in a haystack" within billions of log lines or text documents; visualizing search results.

### 3. Architecture and Working Principle
Talks directly to the Elasticsearch API. Node.js based.
* **Discovery:** Exploring raw data over time.
* **Lens:** Drag-and-drop chart builder.

### 4. Key Components
Index Pattern, KQL (Kibana Query Language).

### 5. Use Cases
Log Analysis, Cyber Security (SIEM).

### 6. Pros and Cons
* **Pros:** Unbeatable for text/log analysis, Native ELK integration.
* **Cons:** Tightly coupled with Elasticsearch (cannot connect to SQL DBs).""",

                "tr": """### 1. TanÄ±m
Kibana; Elasticsearch verileri iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ, Elastic Stack (ELK) ailesinin gÃ¶rselleÅŸtirme ve yÃ¶netim arayÃ¼zÃ¼dÃ¼r.

### 2. Temel AmaÃ§
Milyarlarca satÄ±rlÄ±k log verisi veya metin iÃ§inde aramayÄ± kolaylaÅŸtÄ±rmak ve sonuÃ§larÄ± grafiÄŸe dÃ¶kmektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
DoÄŸrudan Elasticsearch API'si ile konuÅŸur. Node.js tabanlÄ±dÄ±r.
* **Discovery:** Ham veriyi zaman ekseninde inceleme.
* **Lens:** SÃ¼rÃ¼kle-bÄ±rak grafik oluÅŸturma.

### 4. Temel BileÅŸenler
Index Pattern, KQL (Kibana Sorgu Dili).

### 5. KullanÄ±m AlanlarÄ±
Log Analizi, Siber GÃ¼venlik (SIEM).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Metin analizinde rakipsizdir, ELK ile hazÄ±r gelir.
* **Dezavantajlar:** BaÄŸÄ±mlÄ±lÄ±k (Sadece Elasticsearch ile Ã§alÄ±ÅŸÄ±r)."""
            },
            "link": "https://www.elastic.co/kibana", "modern": True, "dep": "Elasticsearch",
            "code": """# KQL Ã–rneÄŸi
status:500 AND host:"web-server-1" AND NOT message:"timeout" """
        },
        "Looker": {
            "desc": {"en": "Enterprise Data Platform.", "tr": "Kurumsal Veri Platformu."},
            "detail": {
                "en": """### 1. Definition
Looker is a cloud-native enterprise data platform with its own modeling language (LookML). Owned by Google.

### 2. Core Purpose
To prevent "Metric Chaos" (different definitions for the same metric). Provides a "Single Source of Truth" via LookML layer.

### 3. Architecture and Working Principle
**In-database** architecture. Looker does not store data; it generates SQL and pushes it to the database (BigQuery, Snowflake).
* **LookML:** A code-based modeling layer that abstracts SQL.

### 4. Key Components
LookML Project, Explore, Looks/Dashboards.

### 5. Use Cases
Data Governance, Embedded Analytics.

### 6. Pros and Cons
* **Pros:** Consistency (Single source of truth), Git integration.
* **Cons:** High Cost, Learning curve (LookML language).""",

                "tr": """### 1. TanÄ±m
Looker; veriyi veritabanÄ±ndan Ã§Ä±karmadan iÅŸleyen, kendine has bir modelleme dili (LookML) olan, bulut tabanlÄ± modern bir kurumsal veri platformudur.

### 2. Temel AmaÃ§
"Metrik KargaÅŸasÄ±"nÄ± Ã¶nlemektir. LookML katmanÄ± sayesinde, bir metrik bir kere tanÄ±mlanÄ±r ve tÃ¼m ÅŸirket o tanÄ±mÄ± kullanÄ±r (Tek GerÃ§eklik KaynaÄŸÄ±).

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Looker'Ä±n kendi veritabanÄ± yoktur. Sorguyu canlÄ± olarak veritabanÄ±na atar.
* **LookML:** SQL'i soyutlayan kod tabanlÄ± modelleme katmanÄ±dÄ±r.

### 4. Temel BileÅŸenler
LookML Projesi, Explore, Dashboards.

### 5. KullanÄ±m AlanlarÄ±
Veri YÃ¶netiÅŸimi, GÃ¶mÃ¼lÃ¼ Analitik (Embedded).

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** TutarlÄ±lÄ±k, Versiyon kontrolÃ¼ (Git).
* **Dezavantajlar:** YÃ¼ksek Maliyet, Ã–ÄŸrenme gereksinimi (LookML)."""
            },
            "link": "https://looker.com/", "modern": True, "dep": None,
            "code": """# LookML Ã–rneÄŸi
view: orders {
  dimension: id {
    primary_key: yes
    type: number
    sql: ${TABLE}.id ;;
  }
  measure: total_revenue {
    type: sum
    sql: ${sale_price} ;;
  }
}"""
        }
    },
    "AI/ML": {
        "MLflow": {
            "desc": {"en": "ML Lifecycle Management.", "tr": "ML YaÅŸam DÃ¶ngÃ¼sÃ¼ YÃ¶netimi."},
            "detail": {
                "en": """### 1. Definition
MLflow is an open-source platform designed to manage the machine learning lifecycle, developed by Databricks.

### 2. Core Purpose
To solve the problems of Experiment Tracking ("Which parameter gave the best result?"), Reproducibility ("Who trained this model?"), and Deployment ("How do I serve this model?").

### 3. Architecture and Working Principle
Modular structure. Can run locally or on a server.
* **Tracking:** Records parameters and metrics.
* **Projects:** Packages code for reproducibility.
* **Models:** Standardizes model packaging.
* **Registry:** Manages model versions (Staging -> Production).

### 4. Key Components
Tracking Server, Model Registry, Projects.

### 5. Use Cases
Experiment Tracking, Model Serving.

### 6. Pros and Cons
* **Pros:** Universal (works with any library), Simple setup.
* **Cons:** Security (RBAC is weak in open-source version).""",

                "tr": """### 1. TanÄ±m
MLflow; makine Ã¶ÄŸrenimi yaÅŸam dÃ¶ngÃ¼sÃ¼nÃ¼ (Lifecycle) yÃ¶netmek iÃ§in geliÅŸtirilmiÅŸ, platformdan baÄŸÄ±msÄ±z, aÃ§Ä±k kaynaklÄ± bir MLOps platformudur. Databricks tarafÄ±ndan geliÅŸtirilmiÅŸtir.

### 2. Temel AmaÃ§
Veri bilimcilerin en bÃ¼yÃ¼k sorunu olan "Hangi parametreyle en iyi sonucu aldÄ±m?", "Bu modeli kim ne zaman eÄŸitti?" ve "Modeli nasÄ±l canlÄ±ya alÄ±rÄ±m?" sorularÄ±nÄ± (Experiment Tracking) Ã§Ã¶zmektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
ModÃ¼ler bir yapÄ±ya sahiptir. Ä°ster tek baÅŸÄ±na (Local), ister bir sunucuda Ã§alÄ±ÅŸabilir. DÃ¶rt ana bileÅŸenden oluÅŸur.

### 4. Temel BileÅŸenler
* **MLflow Tracking:** Deneylerin parametrelerini ve sonuÃ§larÄ±nÄ± kaydeder.
* **MLflow Projects:** Kodu paketleyip her yerde aynÄ± ÅŸekilde Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.
* **MLflow Models:** Modeli farklÄ± formatlarda paketler.
* **Model Registry:** Modellerin versiyonlanmasÄ±nÄ± yÃ¶netir.

### 5. KullanÄ±m AlanlarÄ±
Deney Takibi, Model DaÄŸÄ±tÄ±mÄ±.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Evrensellik (Her kÃ¼tÃ¼phane ile Ã§alÄ±ÅŸÄ±r), Basitlik.
* **Dezavantajlar:** GÃ¼venlik (AÃ§Ä±k kaynak sÃ¼rÃ¼mde yetkilendirme zayÄ±ftÄ±r)."""
            },
            "link": "https://mlflow.org/", "modern": True, "dep": None,
            "code": """import mlflow

# Deney BaÅŸlatma
mlflow.start_run()

# Parametre Kaydetme
mlflow.log_param("learning_rate", 0.01)

# Metrik Kaydetme
mlflow.log_metric("accuracy", 0.95)

# Modeli Kaydetme
mlflow.sklearn.log_model(model, "model")

mlflow.end_run()"""
        },
        "Spark MLlib": {
            "desc": {"en": "Scalable ML Library.", "tr": "Ã–lÃ§eklenebilir ML KÃ¼tÃ¼phanesi."},
            "detail": {
                "en": """### 1. Definition
Spark MLlib is Apache Spark's scalable machine learning library designed to run on large-scale data using in-memory processing.

### 2. Core Purpose
Unlike Scikit-learn which runs on a single machine, MLlib is designed for Distributed Training on Terabytes of data across hundreds of servers.

### 3. Architecture and Working Principle
Built on Spark DataFrames.
* **Transformer:** Algorithm that transforms data.
* **Estimator:** Algorithm that learns from data and produces a model.
* **Pipeline:** Chaining these stages together.

### 4. Key Components
Classification, Regression, Clustering, Collaborative Filtering (ALS).

### 5. Use Cases
Recommendation Systems, Churn Prediction.

### 6. Pros and Cons
* **Pros:** Scalability ( scales with data), Unified stack (ETL + ML).
* **Cons:** Deep Learning support is weak, Latency (Not for real-time inference).""",

                "tr": """### 1. TanÄ±m
Spark MLlib; Apache Spark'Ä±n bellek iÃ§i (in-memory) iÅŸlem gÃ¼cÃ¼nÃ¼ kullanan, bÃ¼yÃ¼k Ã¶lÃ§ekli veriler Ã¼zerinde Ã§alÄ±ÅŸmak iÃ§in tasarlanmÄ±ÅŸ Ã¶lÃ§eklenebilir makine Ã¶ÄŸrenimi kÃ¼tÃ¼phanesidir.

### 2. Temel AmaÃ§
Scikit-learn gibi kÃ¼tÃ¼phaneler tek bir bilgisayarÄ±n RAM'ine sÄ±ÄŸan verilerle Ã§alÄ±ÅŸÄ±rken; MLlib, Terabyte'larca veriyi yÃ¼zlerce sunucuya daÄŸÄ±tarak modelleri eÄŸitmek (Distributed Training) iÃ§in geliÅŸtirilmiÅŸtir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Spark'Ä±n DataFrame yapÄ±sÄ± Ã¼zerine kuruludur.
* **Transformer:** Veriyi dÃ¶nÃ¼ÅŸtÃ¼ren algoritma.
* **Estimator:** Veriden Ã¶ÄŸrenen ve model Ã¼reten algoritma.
* **Pipeline:** Bu parÃ§alarÄ±n zincirleme baÄŸlanmasÄ±.

### 4. Temel BileÅŸenler
SÄ±nÄ±flandÄ±rma, Regresyon, KÃ¼meleme, Ã–neri Sistemleri (ALS).

### 5. KullanÄ±m AlanlarÄ±
Ã–neri Sistemleri (Netflix/Spotify tarzÄ±), MÃ¼ÅŸteri KaybÄ± Tahmini.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Ã–lÃ§eklenebilirlik, Entegrasyon (Veri temizleme ve modelleme aynÄ± yerde).
* **Dezavantajlar:** Derin Ã–ÄŸrenme (ZayÄ±ftÄ±r), Gecikme (AnlÄ±k tahmin iÃ§in hantaldÄ±r)."""
            },
            "link": "https://spark.apache.org/mllib/", "modern": True, "dep": "Spark",
            "code": """from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer

tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)

pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])
model = pipeline.fit(training_data)"""
        },
        "TensorFlow": {
            "desc": {"en": "Deep Learning Framework.", "tr": "Derin Ã–ÄŸrenme KÃ¼tÃ¼phanesi."},
            "detail": {
                "en": """### 1. Definition
TensorFlow is an end-to-end open-source platform for machine learning, developed by Google.

### 2. Core Purpose
To build and deploy complex neural networks for tasks like image recognition and NLP, running on everything from massive servers to mobile devices.

### 3. Architecture and Working Principle
Named after multidimensional data arrays (Tensors) flowing through a computation graph. Supports both Static Graphs and Eager Execution.

### 4. Key Components
Tensor, Keras (High-level API), TensorBoard (Visualization), TF Lite (Mobile).

### 5. Use Cases
Computer Vision, NLP, AlphaGo.

### 6. Pros and Cons
* **Pros:** Production Ready, TFX Ecosystem.
* **Cons:** Steeper learning curve than PyTorch, Debugging static graphs can be hard.""",

                "tr": """### 1. TanÄ±m
TensorFlow; Google tarafÄ±ndan geliÅŸtirilen, derin Ã¶ÄŸrenme ve yapay sinir aÄŸlarÄ± odaklÄ±, uÃ§tan uca aÃ§Ä±k kaynaklÄ± bir makine Ã¶ÄŸrenimi platformudur.

### 2. Temel AmaÃ§
GÃ¶rÃ¼ntÃ¼ iÅŸleme, ses tanÄ±ma gibi karmaÅŸÄ±k problemleri Ã§Ã¶zmek iÃ§in Ã§ok katmanlÄ± sinir aÄŸlarÄ±nÄ± oluÅŸturmak, eÄŸitmek ve bunlarÄ± hem sunucularda hem de mobil cihazlarda Ã§alÄ±ÅŸtÄ±rabilmektir.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Ä°smini verilerin (Tensors) bir iÅŸlem grafiÄŸi boyunca akmasÄ±ndan (Flow) alÄ±r. Hem Statik Grafik hem de Eager Execution (AnÄ±nda Ã§alÄ±ÅŸtÄ±rma) destekler.

### 4. Temel BileÅŸenler
Tensor, Keras (YÃ¼ksek seviyeli API), TensorBoard (GÃ¶rselleÅŸtirme), TensorFlow Lite (Mobil).

### 5. KullanÄ±m AlanlarÄ±
BilgisayarlÄ± GÃ¶ru (Computer Vision), NLP.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** CanlÄ±ya alma (Production) gÃ¼cÃ¼, Ekosistem (TFX).
* **Dezavantajlar:** Ã–ÄŸrenme EÄŸrisi (PyTorch'a gÃ¶re daha diktir)."""
            },
            "link": "https://www.tensorflow.org/", "modern": True, "dep": None,
            "code": """import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])"""
        },
        "PyTorch": {
            "desc": {"en": "Deep Learning Framework.", "tr": "Derin Ã–ÄŸrenme KÃ¼tÃ¼phanesi."},
            "detail": {
                "en": """### 1. Definition
PyTorch is an open-source machine learning library developed by Meta (Facebook) AI. Known for flexibility and speed.

### 2. Core Purpose
To allow researchers to experiment rapidly. It is the standard for academic research and GenAI (LLMs).

### 3. Architecture and Working Principle
Uses **Dynamic Computational Graphs**. The graph is built at runtime, making it very Pythonic and easy to debug.

### 4. Key Components
Torch.nn, Autograd (Automatic differentiation), TorchScript.

### 5. Use Cases
GenAI (LLMs like GPT), Academic Research.

### 6. Pros and Cons
* **Pros:** Ease of use (Pythonic), Debugging, Community.
* **Cons:** Mobile deployment is less mature than TF (but improving).""",

                "tr": """### 1. TanÄ±m
PyTorch; Meta (Facebook) AI Research ekibi tarafÄ±ndan geliÅŸtirilen, esnekliÄŸi ve hÄ±zÄ± ile bilinen, aÃ§Ä±k kaynaklÄ± bir derin Ã¶ÄŸrenme kÃ¼tÃ¼phanesidir.

### 2. Temel AmaÃ§
AraÅŸtÄ±rmacÄ±larÄ±n yeni algoritmalarÄ± hÄ±zlÄ±ca denemelerini saÄŸlamak ve Python doÄŸasÄ±na uygun (Pythonic) bir yapÄ± sunmaktÄ±r. GenAI ve LLM dÃ¼nyasÄ±nÄ±n standardÄ±dÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
**Dinamik Hesaplama GrafiÄŸi** kullanÄ±r. Grafik kod Ã§alÄ±ÅŸÄ±rken oluÅŸturulur. Bu, if-else gibi yapÄ±larÄ± model iÃ§inde kullanmayÄ± kolaylaÅŸtÄ±rÄ±r.

### 4. Temel BileÅŸenler
Torch.nn, Autograd (Otomatik tÃ¼rev alma), TorchScript.

### 5. KullanÄ±m AlanlarÄ±
GenAI (LLM), Akademik AraÅŸtÄ±rma.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** KullanÄ±m KolaylÄ±ÄŸÄ±, Hata AyÄ±klama, Topluluk.
* **Dezavantajlar:** Mobil daÄŸÄ±tÄ±m (TensorFlow kadar olgun deÄŸil)."""
            },
            "link": "https://pytorch.org/", "modern": True, "dep": None,
            "code": """import torch
import torch.nn as nn

# Basit bir Sinir AÄŸÄ±
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"""
        },
        "Kubeflow": {
            "desc": {"en": "ML Toolkit for K8s.", "tr": "Kubernetes ML AraÃ§ Seti."},
            "detail": {
                "en": """### 1. Definition
Kubeflow is a Cloud-Native MLOps toolkit dedicated to making deployments of ML workflows on Kubernetes simple, portable and scalable.

### 2. Core Purpose
To solve the "works on my machine but not on cluster" problem. Combines the ML lifecycle with the power of Kubernetes.

### 3. Architecture and Working Principle
Microservices on top of Kubernetes. Each step (Training, Serving) runs as a separate Pod.

### 4. Key Components
Pipelines, Notebook Servers, Katib (Hyperparameter tuning), KServe.

### 5. Use Cases
Enterprise MLOps, Scalable Training.

### 6. Pros and Cons
* **Pros:** Standardization, Scalability.
* **Cons:** Complexity (Requires K8s expertise), Heavy/Overkill for small teams.""",

                "tr": """### 1. TanÄ±m
Kubeflow; makine Ã¶ÄŸrenimi iÅŸ akÄ±ÅŸlarÄ±nÄ± Kubernetes Ã¼zerinde daÄŸÄ±tmak, Ã¶lÃ§eklendirmek ve yÃ¶netmek iÃ§in geliÅŸtirilmiÅŸ, Cloud-Native bir MLOps araÃ§ setidir.

### 2. Temel AmaÃ§
Model geliÅŸtirme ve daÄŸÄ±tÄ±m sÃ¼recini Kubernetes'in gÃ¼cÃ¼yle birleÅŸtirmek ve standartlaÅŸtÄ±rmaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Kubernetes Ã¼zerine kurulu mikroservislerden oluÅŸur. Her adÄ±m (EÄŸitim, Sunum) ayrÄ± bir Pod olarak Ã§alÄ±ÅŸÄ±r.

### 4. Temel BileÅŸenler
Kubeflow Pipelines, Notebook Servers, Katib (Hiperparametre optimizasyonu), KServe.

### 5. KullanÄ±m AlanlarÄ±
Kurumsal MLOps, Ã–lÃ§eklenebilir EÄŸitim.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** StandartlaÅŸma, Ã–lÃ§eklenebilirlik.
* **Dezavantajlar:** KarmaÅŸÄ±klÄ±k (Kubernetes uzmanlÄ±ÄŸÄ± ister), AÄŸÄ±r kurulum."""
            },
            "link": "https://www.kubeflow.org/", "modern": True, "dep": "Kubernetes",
            "code": """import kfp
from kfp import dsl

@dsl.pipeline(name='My Pipeline')
def my_pipeline():
    train_op = dsl.ContainerOp(
        name='Train',
        image='gcr.io/my-image/train',
        arguments=['--epochs', '50']
    )"""
        },
        "Ray": {
            "desc": {"en": "Distributed AI Compute.", "tr": "DaÄŸÄ±tÄ±k Yapay Zeka Ä°ÅŸlemcisi."},
            "detail": {
                "en": """### 1. Definition
Ray is a unified framework for scaling AI and Python applications.

### 2. Core Purpose
To scale Python code from a single machine to a massive cluster with minimal code changes. Famous for training GPT models (OpenAI).

### 3. Architecture and Working Principle
Uses the **Actor Model**. Creates lightweight tasks and distributes them dynamically across the cluster.

### 4. Key Components
Ray Core, Ray Train, Ray Tune, Ray Serve, RLlib.

### 5. Use Cases
LLM Training, Reinforcement Learning.

### 6. Pros and Cons
* **Pros:** Pythonic, Performance (Low latency), Flexibility.
* **Cons:** Memory management in large clusters can be tricky.""",

                "tr": """### 1. TanÄ±m
Ray; Python uygulamalarÄ±nÄ± ve Ã¶zellikle yapay zeka iÅŸ yÃ¼klerini Ã¶lÃ§eklendirmek iÃ§in geliÅŸtirilmiÅŸ, birleÅŸik bir daÄŸÄ±tÄ±k hesaplama Ã§erÃ§evesidir.

### 2. Temel AmaÃ§
Tek bir bilgisayarda yazÄ±lan Python kodunu, neredeyse hiÃ§ deÄŸiÅŸtirmeden binlerce sunucudan oluÅŸan bir kÃ¼meye yaymaktÄ±r.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
**Actor Model** kullanÄ±r. Hafif gÃ¶revler oluÅŸturur ve bunlarÄ± dinamik olarak sunuculara daÄŸÄ±tÄ±r.

### 4. Temel BileÅŸenler
Ray Core, Ray Train, Ray Tune, Ray Serve, RLlib.

### 5. KullanÄ±m AlanlarÄ±
LLM EÄŸitimi (GPT), PekiÅŸtirmeli Ã–ÄŸrenme.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** Pythonic (Kolay Ã¶ÄŸrenilir), Performans, Esneklik.
* **Dezavantajlar:** Kaynak YÃ¶netimi (BÃ¼yÃ¼k kÃ¼melerde zorlaÅŸabilir)."""
            },
            "link": "https://www.ray.io/", "modern": True, "dep": None,
            "code": """import ray

ray.init()

@ray.remote
def f(x):
    return x * x

futures = [f.remote(i) for i in range(4)]
print(ray.get(futures))"""
        },
        "Hugging Face": {
            "desc": {"en": "The AI Community.", "tr": "Yapay Zeka Topluluk Platformu."},
            "detail": {
                "en": """### 1. Definition
Hugging Face is the "GitHub of AI". A platform for sharing and collaborating on ML models, datasets, and demos.

### 2. Core Purpose
To democratize AI. Making state-of-the-art models (BERT, GPT) accessible to everyone.

### 3. Architecture and Working Principle
Central Model Hub and open-source libraries (Transformers).

### 4. Key Components
Model Hub, Datasets, Spaces (Demos), Transformers Library.

### 5. Use Cases
Transfer Learning, NLP & Vision tasks.

### 6. Pros and Cons
* **Pros:** Accessibility, Standardization.
* **Cons:** Hardware requirements for running large models.""",

                "tr": """### 1. TanÄ±m
Hugging Face; yapay zeka modellerini paylaÅŸmak, keÅŸfetmek ve kullanmak iÃ§in oluÅŸturulmuÅŸ, "AI dÃ¼nyasÄ±nÄ±n GitHub'Ä±" olarak bilinen platformdur.

### 2. Temel AmaÃ§
Makine Ã¶ÄŸrenimini demokratikleÅŸtirmek. Devasa modelleri (BERT, GPT) herkesin kullanÄ±mÄ±na sunmak.

### 3. Mimari ve Ã‡alÄ±ÅŸma Prensibi
Merkezi bir Model Deposu (Hub) ve aÃ§Ä±k kaynak kÃ¼tÃ¼phanelerden oluÅŸur.

### 4. Temel BileÅŸenler
Model Hub, Datasets, Spaces, Transformers KÃ¼tÃ¼phanesi.

### 5. KullanÄ±m AlanlarÄ±
Transfer Learning (HazÄ±r model kullanÄ±mÄ±), NLP ve GÃ¶rÃ¼ntÃ¼ iÅŸleme.

### 6. Avantajlar ve Dezavantajlar
* **Avantajlar:** EriÅŸilebilirlik, StandartlaÅŸma.
* **Dezavantajlar:** DonanÄ±m (BÃ¼yÃ¼k modeller GPU ister)."""
            },
            "link": "https://huggingface.co/", "modern": True, "dep": None,
            "code": """from transformers import pipeline

# Duygu Analizi
classifier = pipeline("sentiment-analysis")
result = classifier("I love using Hugging Face!")
print(result)"""
        }

    }
}

# --- YARDIMCI FONKSÄ°YONLAR ---
CATEGORY_COLORS = {
    "Ingestion": "#90EE90", "Storage": "#FFD700", "Lakehouse": "#00CED1",
    "Processing": "#FF6347", "Databases": "#ADD8E6", "Serving/BI": "#DDA0DD",
    "Orchestration": "#D3D3D3", "AI/ML": "#FF69B4"
}


def get_category(node_name):
    for cat, items in TECH_STACK.items():
        if node_name in items:
            return cat
    return "Unknown"


def validate_stack(selected_nodes, lang="en"):
    errors = []
    for tech in selected_nodes:
        if tech in DEPENDENCY_RULES:
            required_deps = DEPENDENCY_RULES[tech]
            for dep in required_deps:
                if dep not in selected_nodes:
                    msg_template = UI_TEXTS[lang].get("error_missing_dep", "Missing dependency: {tech} needs {dep}")
                    errors.append(msg_template.format(tech=tech, dep=dep))
    return errors


def auto_connect_nodes(selected_nodes):
    layered_nodes = {0: [], 1: [], 2: [], 3: [], 4: [], 5: []}
    for node in selected_nodes:
        cat = get_category(node)
        priority = LAYER_PRIORITY.get(cat, 3)
        layered_nodes[priority].append(node)

    edges = []
    active_layers = [i for i in range(1, 6) if layered_nodes[i]]
    for i in range(len(active_layers) - 1):
        current_idx = active_layers[i]
        next_idx = active_layers[i + 1]
        sources = layered_nodes[current_idx]
        targets = layered_nodes[next_idx]
        for s in sources:
            for t in targets:
                edges.append((s, t))
    return layered_nodes, edges